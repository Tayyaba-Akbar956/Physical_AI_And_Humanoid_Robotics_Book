"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[3962],{8396:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-05-humanoid-control/part-01-locomotion/humanoid-overview","title":"Humanoid Robotics Overview","description":"This chapter provides a comprehensive introduction to humanoid robotics, exploring the unique challenges and opportunities presented by robots that resemble and operate similarly to humans. Humanoid robots represent one of the most ambitious areas of robotics research, combining complex mechanical design, advanced control systems, and sophisticated AI to create machines that can operate in human-centered environments.","source":"@site/docs/module-05-humanoid-control/part-01-locomotion/03-humanoid-overview.md","sourceDirName":"module-05-humanoid-control/part-01-locomotion","slug":"/module-05-humanoid-control/part-01-locomotion/humanoid-overview","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-05-humanoid-control/part-01-locomotion/humanoid-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba-Akbar956/Physical_AI_And_Humanoid_Robotics_Book/tree/main/docs/module-05-humanoid-control/part-01-locomotion/03-humanoid-overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Humanoid Robotics Overview"},"sidebar":"tutorialSidebar","previous":{"title":"Bipedal Locomotion","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-05-humanoid-control/part-01-locomotion/bipedal-locomotion"},"next":{"title":"Balance Control","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-05-humanoid-control/part-02-interaction/manipulation"}}');var t=i(4848),r=i(8453);const s={sidebar_position:1,title:"Humanoid Robotics Overview"},a="Humanoid Robotics Overview",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction: The Quest to Build Human-like Robots",id:"introduction-the-quest-to-build-human-like-robots",level:2},{value:"Key Characteristics of Humanoid Robots",id:"key-characteristics-of-humanoid-robots",level:3},{value:"Challenges in Humanoid Robotics",id:"challenges-in-humanoid-robotics",level:3},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Mechanical Design Principles",id:"mechanical-design-principles",level:3},{value:"Control Architecture",id:"control-architecture",level:3},{value:"Cognitive Capabilities",id:"cognitive-capabilities",level:3},{value:"Types of Humanoid Robots",id:"types-of-humanoid-robots",level:2},{value:"Research Platforms",id:"research-platforms",level:3},{value:"Commercial Applications",id:"commercial-applications",level:3},{value:"Technical Classifications",id:"technical-classifications",level:3},{value:"Mathematical Framework",id:"mathematical-framework",level:2},{value:"Kinematic Models",id:"kinematic-models",level:3},{value:"Dynamic Equations",id:"dynamic-equations",level:3},{value:"Balance and Locomotion Models",id:"balance-and-locomotion-models",level:3},{value:"Practical Implementation",id:"practical-implementation",level:2},{value:"Humanoid Robot Control Architecture",id:"humanoid-robot-control-architecture",level:3},{value:"Bipedal Locomotion Implementation",id:"bipedal-locomotion-implementation",level:3},{value:"Manipulation Control",id:"manipulation-control",level:3},{value:"Advanced Topics",id:"advanced-topics",level:2},{value:"Whole-Body Control",id:"whole-body-control",level:3},{value:"Learning-Based Approaches",id:"learning-based-approaches",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Balance Problems",id:"balance-problems",level:3},{value:"Locomotion Issues",id:"locomotion-issues",level:3},{value:"Manipulation Challenges",id:"manipulation-challenges",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Design Considerations",id:"design-considerations",level:3},{value:"Control Strategies",id:"control-strategies",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"humanoid-robotics-overview",children:"Humanoid Robotics Overview"})}),"\n",(0,t.jsx)(e.p,{children:"This chapter provides a comprehensive introduction to humanoid robotics, exploring the unique challenges and opportunities presented by robots that resemble and operate similarly to humans. Humanoid robots represent one of the most ambitious areas of robotics research, combining complex mechanical design, advanced control systems, and sophisticated AI to create machines that can operate in human-centered environments."}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the principles and challenges of humanoid robotics"}),"\n",(0,t.jsx)(e.li,{children:"Analyze the mechanical, control, and cognitive aspects of humanoid design"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate the applications and potential of humanoid robots"}),"\n",(0,t.jsx)(e.li,{children:"Recognize the interdisciplinary nature of humanoid robotics"}),"\n",(0,t.jsx)(e.li,{children:"Compare humanoid designs and their respective advantages"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"introduction-the-quest-to-build-human-like-robots",children:"Introduction: The Quest to Build Human-like Robots"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robotics seeks to create artificial beings that possess human-like form, movement capabilities, and eventually, human-like intelligence. This endeavor is driven by multiple motivations:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Human-like Interaction"}),": Robots that can interact with human-designed environments and interfaces"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Social Acceptance"}),": Human-like robots may be more readily accepted by humans"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Versatility"}),": Human-like form factor can navigate environments designed for humans"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Research"}),": Understanding human movement and cognition through robotic implementation"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"key-characteristics-of-humanoid-robots",children:"Key Characteristics of Humanoid Robots"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Bipedal Locomotion"}),": Human-like walking on two legs, requiring sophisticated balance control\n",(0,t.jsx)(e.strong,{children:"Upper Limb Manipulation"}),": Human-like arms and hands for manipulation tasks\n",(0,t.jsx)(e.strong,{children:"Anthropomorphic Form"}),": Human-like proportions and structure\n",(0,t.jsx)(e.strong,{children:"Social Cognition"}),": Ability to understand and respond to social cues\n",(0,t.jsx)(e.strong,{children:"Embodied Intelligence"}),": Integration of perception, cognition, and action"]}),"\n",(0,t.jsx)(e.h3,{id:"challenges-in-humanoid-robotics",children:"Challenges in Humanoid Robotics"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Balance and Stability"}),": Maintaining balance while moving and performing tasks\n",(0,t.jsx)(e.strong,{children:"Complex Kinematics"}),": Managing the high-dimensional movement space\n",(0,t.jsx)(e.strong,{children:"Real-time Control"}),": Processing sensory information and responding in real-time\n",(0,t.jsx)(e.strong,{children:"Energy Efficiency"}),": Operating efficiently for practical applications\n",(0,t.jsx)(e.strong,{children:"Robustness"}),": Handling unpredictable real-world situations"]}),"\n",(0,t.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,t.jsx)(e.h3,{id:"mechanical-design-principles",children:"Mechanical Design Principles"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Degrees of Freedom"}),": Humanoid robots typically have 20-50+ degrees of freedom distributed throughout the body"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Legs"}),": 6 DOF each (hip: 3 DOF, knee: 1 DOF, ankle: 2 DOF)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Arms"}),": 7 DOF each (shoulder: 3 DOF, elbow: 1 DOF, wrist: 3 DOF)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Torso"}),": 3-6 DOF for flexibility"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Neck/Head"}),": 2-3 DOF for gaze control"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Actuation Systems"}),": Various approaches to joint actuation:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Servo Motors"}),": Precise position control with feedback"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Series Elastic Actuators"}),": Compliance for safer human interaction"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Pneumatic Muscles"}),": Human-like compliance and force control"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hydraulic Systems"}),": High force output for heavy-duty applications"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Sensing Systems"}),": Multiple sensor types for environmental awareness:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Proprioceptive Sensors"}),": Joint position, velocity, and torque feedback"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"IMU Systems"}),": Acceleration, angular velocity, and orientation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Force/Torque Sensors"}),": Interaction forces with environment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision Systems"}),": Cameras for environmental perception"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Tactile Sensors"}),": Contact and pressure detection"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"control-architecture",children:"Control Architecture"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Hierarchical Control"}),": Multi-level control architecture:"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"High-Level Planning"}),": Task decomposition and sequence planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Mid-Level Control"}),": Trajectory generation and balance control"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Low-Level Control"}),": Joint servo control and motor commands"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Balance Control"}),": Maintaining stability during static and dynamic tasks:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Zero-Moment Point (ZMP)"}),": Dynamic balance criterion"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Capture Point"}),": Method for balance recovery"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Center of Mass (CoM) Control"}),": Managing whole-body stability"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Locomotion Control"}),": Algorithms for bipedal walking:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Inverted Pendulum Models"}),": Simplified balance control"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Central Pattern Generators"}),": Rhythmic movement patterns"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Model Predictive Control"}),": Predictive control approaches"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"cognitive-capabilities",children:"Cognitive Capabilities"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Perception Systems"}),": Processing multi-sensory information:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visual Processing"}),": Object recognition, scene understanding"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Audio Processing"}),": Speech recognition, sound localization"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Tactile Processing"}),": Contact and force sensing"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Reasoning and Planning"}),": High-level cognitive functions:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Planning"}),": Decomposing complex tasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Motion Planning"}),": Generating collision-free movement trajectories"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Learning"}),": Adapting to new situations and improving performance"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"types-of-humanoid-robots",children:"Types of Humanoid Robots"}),"\n",(0,t.jsx)(e.h3,{id:"research-platforms",children:"Research Platforms"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Honda ASIMO"}),": One of the most famous bipedal robots, showcasing advanced walking and interaction capabilities\n",(0,t.jsx)(e.strong,{children:"Boston Dynamics Atlas"}),": High mobility platform for dynamic tasks\n",(0,t.jsx)(e.strong,{children:"NASA Valkyrie"}),": Designed for space applications with dexterity and autonomy\n",(0,t.jsx)(e.strong,{children:"NAO by SoftBank Robotics"}),": Small humanoid for education and research"]}),"\n",(0,t.jsx)(e.h3,{id:"commercial-applications",children:"Commercial Applications"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Pepper by SoftBank"}),": Customer service and companion applications\n",(0,t.jsx)(e.strong,{children:"Sophia by Hanson Robotics"}),": Social interaction and research platform\n",(0,t.jsx)(e.strong,{children:"Toyota HRP-4"}),": Entertainment and demonstration purposes"]}),"\n",(0,t.jsx)(e.h3,{id:"technical-classifications",children:"Technical Classifications"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"By Mobility"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fixed Base"}),": Torso fixed to stable platform, only arms moving"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Mobile Base"}),": Wheeled or tracked mobility with stationary upper body"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Bipedal"}),": True two-legged walking capability"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"By Dexterity"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simple Arms"}),": Basic manipulation with limited DOF"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Anthropomorphic Hands"}),": Human-like hand with individual finger control"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Advanced Manipulation"}),": Full arm/shoulder/torso coordination"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"By Cognitive Capability"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reactive"}),": Simple stimulus-response behaviors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Autonomous"}),": Independent task execution with basic learning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Social"}),": Advanced interaction and adaptive behavior"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"mathematical-framework",children:"Mathematical Framework"}),"\n",(0,t.jsx)(e.h3,{id:"kinematic-models",children:"Kinematic Models"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robots are modeled as multi-body systems where each link is connected through joints. The kinematic model describes the geometric relationships between the robot's joints and end-effectors."}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Forward Kinematics"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"T_n = T_0 * A_1(\u03b8_1) * A_2(\u03b8_2) * ... * A_n(\u03b8_n)\n"})}),"\n",(0,t.jsx)(e.p,{children:"Where:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"T_n"})," is the transformation matrix to the end-effector"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"T_0"})," is the base transformation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"A_i(\u03b8_i)"})," is the transformation due to joint i with angle \u03b8_i"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Inverse Kinematics"}),": Solving for joint angles given desired end-effector position:"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u03b8 = f^(-1)(x_d)\n"})}),"\n",(0,t.jsx)(e.p,{children:"Where:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"x_d"})," is the desired end-effector pose"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"f"})," is the forward kinematics function"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03b8"})," is the vector of joint angles"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"dynamic-equations",children:"Dynamic Equations"}),"\n",(0,t.jsx)(e.p,{children:"For control purposes, humanoid robots are often modeled using the Lagrange equation of motion:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"M(q)q\u0308 + C(q,q\u0307)q\u0307 + g(q) = \u03c4\n"})}),"\n",(0,t.jsx)(e.p,{children:"Where:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"M(q)"})," is the mass matrix"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"C(q,q\u0307)"})," is the Coriolis and centrifugal forces matrix"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"g(q)"})," is the gravity vector"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"q"}),", ",(0,t.jsx)(e.code,{children:"q\u0307"}),", ",(0,t.jsx)(e.code,{children:"q\u0308"})," are joint positions, velocities, and accelerations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03c4"})," is the vector of joint torques"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"balance-and-locomotion-models",children:"Balance and Locomotion Models"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Zero-Moment Point (ZMP)"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"x_zmp = x_com - (h/g) * \u1e8d_com\ny_zmp = y_com - (h/g) * \xff_com\n"})}),"\n",(0,t.jsx)(e.p,{children:"Where:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"(x_com, y_com)"})," is the center of mass position"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"h"})," is the CoM height"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"g"})," is gravitational acceleration"]}),"\n",(0,t.jsx)(e.li,{children:"Dots denote derivatives"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"The ZMP must remain within the support polygon for stable walking."}),"\n",(0,t.jsx)(e.h2,{id:"practical-implementation",children:"Practical Implementation"}),"\n",(0,t.jsx)(e.h3,{id:"humanoid-robot-control-architecture",children:"Humanoid Robot Control Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nimport time\n\n@dataclass\nclass JointState:\n    position: float\n    velocity: float\n    effort: float\n\n@dataclass\nclass RobotState:\n    joint_states: Dict[str, JointState]\n    base_pose: np.ndarray  # [x, y, z, qw, qx, qy, qz]\n    center_of_mass: np.ndarray  # [x, y, z]\n    zmp: np.ndarray  # [x, y]\n    support_polygon: List[np.ndarray]  # Vertices of polygon\n\nclass HumanoidController:\n    def __init__(self, config):\n        """\n        Initialize the humanoid robot controller\n        \n        Args:\n            config: Configuration dictionary for the robot\n        """\n        self.config = config\n        self.current_state = RobotState(\n            joint_states={},\n            base_pose=np.zeros(7),\n            center_of_mass=np.zeros(3),\n            zmp=np.zeros(2),\n            support_polygon=[]\n        )\n        \n        # Control parameters\n        self.control_freq = config.get(\'control_frequency\', 200)  # Hz\n        self.dt = 1.0 / self.control_freq\n        \n        # Balance control parameters\n        self.zmp_margin = config.get(\'zmp_margin\', 0.02)  # 2cm safety margin\n        self.com_height = config.get(\'com_height\', 0.8)  # m\n        self.gravity = 9.81  # m/s^2\n        \n        # PID controllers for joints\n        self.joint_controllers = {}\n        for joint_name in config.get(\'joint_names\', []):\n            self.joint_controllers[joint_name] = PIDController(\n                kp=config.get(\'kp\', 100),\n                ki=config.get(\'ki\', 1),\n                kd=config.get(\'kd\', 10)\n            )\n    \n    def update_state(self, new_state: RobotState):\n        """Update the robot\'s state with new sensor information"""\n        self.current_state = new_state\n        self._update_balance_metrics()\n    \n    def _update_balance_metrics(self):\n        """Update balance-related metrics like ZMP from current state"""\n        # Calculate ZMP from current CoM state\n        # ZMP = CoM_xy - (h/g) * CoM_acc_xy\n        # In practice, this would use IMU and force/torque sensor data\n        \n        # Simplified calculation assuming CoM acceleration is available\n        # In real systems, ZMP is typically calculated from foot force sensors\n        pass\n    \n    def is_balanced(self):\n        """Check if the robot is currently balanced"""\n        # Check if ZMP is within support polygon\n        if not self.current_state.support_polygon:\n            return False  # No support polygon defined\n        \n        # Check if ZMP is within the support polygon\n        return self._is_point_in_polygon(\n            self.current_state.zmp, \n            self.current_state.support_polygon\n        )\n    \n    def _is_point_in_polygon(self, point, polygon):\n        """Check if a point is inside the polygon"""\n        # Ray casting algorithm implementation\n        x, y = point\n        n = len(polygon)\n        inside = False\n        \n        p1x, p1y = polygon[0]\n        for i in range(1, n + 1):\n            p2x, p2y = polygon[i % n]\n            if y > min(p1y, p2y):\n                if y <= max(p1y, p2y):\n                    if x <= max(p1x, p2x):\n                        if p1y != p2y:\n                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n                        if p1x == p2x or x <= xinters:\n                            inside = not inside\n            p1x, p1y = p2x, p2y\n        \n        return inside\n    \n    def compute_balance_control(self, target_zmp: Optional[np.ndarray] = None):\n        """\n        Compute control actions to maintain balance\n        \n        Args:\n            target_zmp: Desired ZMP position (if None, use current balance)\n        \n        Returns:\n            Dictionary of control commands for joints\n        """\n        if target_zmp is None:\n            # Use current ZMP as target for balancing\n            target_zmp = self.current_state.zmp\n        \n        # Calculate ZMP error\n        zmp_error = target_zmp - self.current_state.zmp\n        \n        # Generate control command based on ZMP error\n        # This is a simplified approach - real systems would use more complex models\n        control_commands = {}\n        \n        # Example: Move feet to adjust balance\n        # In practice, this could involve ankle torques, hip adjustments, etc.\n        if np.linalg.norm(zmp_error) > self.zmp_margin:\n            # Generate corrective action\n            control_commands = self._generate_balance_correction(zmp_error)\n        \n        return control_commands\n    \n    def _generate_balance_correction(self, zmp_error):\n        """Generate balance correction commands"""\n        # Simplified balance correction algorithm\n        # In reality, this would use model-based control, MPC, or other advanced methods\n        commands = {}\n        \n        # Example: Adjust ankle joints to shift ZMP\n        # This would be specific to the robot\'s configuration\n        if \'l_ankle_pitch\' in self.joint_controllers:\n            # Calculate desired ankle movement to correct ZMP\n            ankle_correction = 0.1 * zmp_error[0]  # Simplified\n            commands[\'l_ankle_pitch\'] = ankle_correction\n            commands[\'r_ankle_pitch\'] = ankle_correction\n        \n        if \'l_ankle_roll\' in self.joint_controllers:\n            ankle_correction = 0.1 * zmp_error[1]  # Simplified\n            commands[\'l_ankle_roll\'] = ankle_correction\n            commands[\'r_ankle_roll\'] = -ankle_correction  # Opposite for stability\n        \n        return commands\n    \n    def compute_locomotion_control(self, target_velocity: np.ndarray):\n        """\n        Compute control for bipedal locomotion\n        \n        Args:\n            target_velocity: Desired velocity [x, y, theta] in base frame\n        \n        Returns:\n            Control commands for walking gait\n        """\n        # Implement walking pattern generator\n        # This would include: step planning, foot placement, gait generation\n        pass\n    \n    def compute_manipulation_control(self, target_ee_pose: Dict[str, np.ndarray]):\n        """\n        Compute control for arm manipulation tasks\n        \n        Args:\n            target_ee_pose: Target end-effector poses for each manipulator\n        \n        Returns:\n            Joint commands for manipulation\n        """\n        # Solve inverse kinematics for target poses\n        # Apply redundancy resolution if needed\n        pass\n\nclass PIDController:\n    def __init__(self, kp=1.0, ki=0.0, kd=0.0, output_limits=(-np.inf, np.inf)):\n        """\n        Simple PID controller for joint control\n        \n        Args:\n            kp: Proportional gain\n            ki: Integral gain\n            kd: Derivative gain\n            output_limits: Limits for control output\n        """\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.output_limits = output_limits\n        \n        self.reset()\n    \n    def reset(self):\n        """Reset the PID controller"""\n        self._last_error = 0.0\n        self._integral = 0.0\n    \n    def compute(self, error, dt):\n        """\n        Compute control output\n        \n        Args:\n            error: Position error\n            dt: Time step\n        \n        Returns:\n            Control output\n        """\n        # Proportional term\n        p_term = self.kp * error\n        \n        # Integral term\n        self._integral += error * dt\n        i_term = self.ki * self._integral\n        \n        # Derivative term\n        if dt > 0:\n            derivative = (error - self._last_error) / dt\n        else:\n            derivative = 0.0\n        d_term = self.kd * derivative\n        \n        # Total output\n        output = p_term + i_term + d_term\n        \n        # Apply output limits\n        output = np.clip(output, self.output_limits[0], self.output_limits[1])\n        \n        # Update last error\n        self._last_error = error\n        \n        return output\n\n# Example usage\nif __name__ == "__main__":\n    # Define robot configuration\n    robot_config = {\n        \'control_frequency\': 200,\n        \'joint_names\': [\'l_hip_yaw\', \'l_hip_roll\', \'l_hip_pitch\', \n                       \'l_knee\', \'l_ankle_pitch\', \'l_ankle_roll\',\n                       \'r_hip_yaw\', \'r_hip_roll\', \'r_hip_pitch\',\n                       \'r_knee\', \'r_ankle_pitch\', \'r_ankle_roll\'],\n        \'zmp_margin\': 0.02,\n        \'com_height\': 0.8\n    }\n    \n    controller = HumanoidController(robot_config)\n    \n    # Simulate state updates\n    for i in range(100):\n        # In a real system, this would come from sensors\n        sim_state = RobotState(\n            joint_states={},  # Simulated joint states\n            base_pose=np.array([0, 0, 0, 1, 0, 0, 0]),\n            center_of_mass=np.array([0.01, 0.02, 0.8]),  # Slightly off-center\n            zmp=np.array([-0.01, 0.01]),  # Slightly off target\n            support_polygon=[np.array([-0.1, -0.05]), \n                           np.array([0.1, -0.05]), \n                           np.array([0.1, 0.05]), \n                           np.array([-0.1, 0.05])]\n        )\n        \n        controller.update_state(sim_state)\n        \n        # Compute balance control\n        balance_commands = controller.compute_balance_control()\n        print(f"Balance commands: {balance_commands}")\n        \n        # Wait for next control cycle\n        time.sleep(1/controller.control_freq)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"bipedal-locomotion-implementation",children:"Bipedal Locomotion Implementation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class WalkingPatternGenerator:\n    def __init__(self, robot_properties):\n        \"\"\"\n        Generate walking patterns for bipedal locomotion\n        \n        Args:\n            robot_properties: Dictionary with robot-specific parameters\n        \"\"\"\n        self.step_length = robot_properties.get('step_length', 0.3)  # m\n        self.step_width = robot_properties.get('step_width', 0.2)   # m\n        self.step_height = robot_properties.get('step_height', 0.1) # m\n        self.step_duration = robot_properties.get('step_duration', 1.0)  # s\n        self.nominal_com_height = robot_properties.get('com_height', 0.8)\n        \n        # Walking parameters\n        self.stride_phase_ratio = 0.6  # Ratio of stride phase to step phase\n        self.dsp_percentage = robot_properties.get('dsp_percentage', 0.2)  # Double Support Phase %\n        self.pelvis_oscillation = robot_properties.get('pelvis_oscillation', 0.02)  # Pelvis oscillation in m\n    \n    def generate_walking_pattern(self, num_steps, walk_velocity=[0.2, 0.0, 0.0]):\n        \"\"\"\n        Generate a complete walking pattern for the specified number of steps\n        \n        Args:\n            num_steps: Number of steps to generate\n            walk_velocity: Desired walking velocity [x_vel, y_vel, theta_vel]\n        \n        Returns:\n            Walking pattern with foot placements and trajectories\n        \"\"\"\n        walking_pattern = {\n            'step_sequence': [],\n            'com_trajectory': [],\n            'zmp_trajectory': [],\n            'foot_trajectories': []\n        }\n        \n        # Start position\n        current_pos = np.array([0.0, 0.0, 0.0])  # x, y, theta\n        current_support_foot = 'right'  # Start with right foot support\n        \n        for step_idx in range(num_steps):\n            # Determine step direction based on walking velocity and phase\n            step_direction = self._calculate_step_direction(walk_velocity, step_idx)\n            \n            # Calculate foot placement\n            foot_placement = self._calculate_foot_placement(\n                current_pos, current_support_foot, step_direction\n            )\n            \n            # Generate step trajectory\n            step_trajectory = self._generate_single_step_trajectory(\n                current_support_foot, foot_placement, current_pos\n            )\n            \n            # Calculate CoM trajectory for this step\n            com_trajectory = self._generate_com_trajectory(\n                step_trajectory, self.nominal_com_height\n            )\n            \n            # Calculate desired ZMP trajectory\n            zmp_trajectory = self._calculate_zmp_trajectory(com_trajectory)\n            \n            # Add to pattern\n            walking_pattern['step_sequence'].append({\n                'step_number': step_idx,\n                'support_foot': current_support_foot,\n                'swing_foot_placement': foot_placement,\n                'step_trajectory': step_trajectory\n            })\n            \n            walking_pattern['com_trajectory'].extend(com_trajectory)\n            walking_pattern['zmp_trajectory'].extend(zmp_trajectory)\n            \n            # Update for next step\n            current_pos = self._update_robot_position(current_pos, foot_placement, current_support_foot)\n            current_support_foot = 'left' if current_support_foot == 'right' else 'right'\n        \n        return walking_pattern\n    \n    def _calculate_step_direction(self, walk_velocity, step_idx):\n        \"\"\"\n        Calculate the direction for the next step based on desired velocity\n        \"\"\"\n        # For now, use a simple model - in reality this would be more complex\n        # considering the robot's stability and gait pattern\n        direction = np.array([walk_velocity[0], walk_velocity[1]])\n        if np.linalg.norm(direction) < 1e-6:  # Very slow or stopped\n            direction = np.array([self.step_length, 0])  # Move forward\n        \n        return direction / np.linalg.norm(direction)  # Normalize\n    \n    def _calculate_foot_placement(self, current_pos, support_foot, step_direction):\n        \"\"\"\n        Calculate where to place the next swing foot\n        \"\"\"\n        # Calculate nominal foot placement based on step parameters\n        if support_foot == 'right':\n            # Next placement should be for left foot\n            lateral_offset = self.step_width / 2\n        else:\n            # Next placement should be for right foot\n            lateral_offset = -self.step_width / 2\n        \n        # Calculate position based on current robot state and desired direction\n        step_distance = self.step_length\n        dx = step_distance * step_direction[0]\n        dy = step_distance * step_direction[1]\n        \n        # Add lateral offset for alternating foot placement\n        foot_pos = current_pos[:2] + np.array([dx, dy]) + np.array([-dy * 0.1, lateral_offset * 1.1])\n        \n        return np.append(foot_pos, 0)  # Add z-coordinate\n    \n    def _generate_single_step_trajectory(self, support_foot, target_pos, robot_pos):\n        \"\"\"\n        Generate a trajectory for a single step using foot lifting and lowering\n        \"\"\"\n        # Define step phases: lift, swing, lower\n        time_points = np.linspace(0, self.step_duration, int(self.step_duration * 200))  # 200 Hz\n        \n        # Get initial position\n        start_pos = robot_pos[:2]  # Simplified - in reality would get from robot state\n        if support_foot == 'right':\n            initial_offset = np.array([0, self.step_width / 2])\n        else:\n            initial_offset = np.array([0, -self.step_width / 2])\n        \n        # Actual start position of swing foot\n        actual_start = start_pos + initial_offset\n        \n        trajectory = []\n        for t in time_points:\n            # Calculate phase progress (0 to 1)\n            progress = t / self.step_duration\n            \n            # Calculate horizontal trajectory (cubic interpolation for smooth motion)\n            if progress < 0.5:  # First half: lift and move forward\n                x = actual_start[0] + (target_pos[0] - actual_start[0]) * progress * 2\n                y = actual_start[1] + (target_pos[1] - actual_start[1]) * progress * 2\n                # Lift foot\n                z = self.step_height * np.sin(np.pi * progress)  # Sinusoidal lift\n            else:  # Second half: continue forward and lower foot\n                second_half_progress = (progress - 0.5) * 2  # Normalize to 0-1\n                x = actual_start[0] + (target_pos[0] - actual_start[0]) * (0.5 + 0.5 * second_half_progress)\n                y = actual_start[1] + (target_pos[1] - actual_start[1]) * (0.5 + 0.5 * second_half_progress)\n                # Lower foot\n                z = self.step_height * np.sin(np.pi * (0.5 + 0.5 * second_half_progress))\n            \n            trajectory.append(np.array([x, y, z]))\n        \n        return trajectory\n    \n    def _generate_com_trajectory(self, step_trajectory, com_height):\n        \"\"\"\n        Generate CoM trajectory that maintains balance during the step\n        \"\"\"\n        # For simplicity, we'll use an inverted pendulum model\n        # In reality, this would be much more complex involving whole-body control\n        com_trajectory = []\n        \n        for foot_pos in step_trajectory:\n            # Simplified: CoM stays roughly above the middle between feet\n            # This is a very simplified approach\n            com_x = foot_pos[0] - 0.05  # Keep CoM slightly behind foot for stability\n            com_y = foot_pos[1]  # Match foot's lateral position for balance\n            com_z = com_height  # Maintain constant height\n            \n            com_trajectory.append(np.array([com_x, com_y, com_z]))\n        \n        return com_trajectory\n    \n    def _calculate_zmp_trajectory(self, com_trajectory):\n        \"\"\"\n        Calculate the desired ZMP trajectory from CoM trajectory\n        \"\"\"\n        zmp_trajectory = []\n        \n        # Using the simplified relationship: ZMP_x = CoM_x - (h/g) * CoM_acc_x\n        # For this example, we'll assume a quasi-static model\n        for com_pos in com_trajectory:\n            # In quasi-static model, ZMP \u2248 CoM projected to ground\n            zmp_x = com_pos[0]  # - (h/g) * 0 (assuming no acceleration)\n            zmp_y = com_pos[1]  # - (h/g) * 0\n            zmp_trajectory.append(np.array([zmp_x, zmp_y, 0]))\n        \n        return zmp_trajectory\n    \n    def _update_robot_position(self, current_pos, foot_placement, support_foot):\n        \"\"\"\n        Update robot's position after taking a step\n        \"\"\"\n        # Calculate new position based on where the foot was placed\n        new_pos = current_pos.copy()\n        \n        # Simplified update - in reality this would involve full kinematic chain\n        if support_foot == 'right':\n            # Left foot becomes new support, so position is affected by left foot placement\n            new_pos[0] = foot_placement[0]  # Update x based on step\n            new_pos[1] = foot_placement[1]  # Update y based on step\n        else:\n            # Right foot becomes new support\n            new_pos[0] = foot_placement[0]\n            new_pos[1] = foot_placement[1]\n        \n        return new_pos\n\n# Example usage\nif __name__ == \"__main__\":\n    # Robot properties for a typical humanoid\n    robot_props = {\n        'step_length': 0.3,      # 30cm\n        'step_width': 0.2,       # 20cm between feet\n        'step_height': 0.05,     # 5cm lift\n        'step_duration': 1.0,    # 1 second per step\n        'com_height': 0.8,       # 80cm CoM height\n        'dsp_percentage': 0.1    # 10% double support\n    }\n    \n    walker = WalkingPatternGenerator(robot_props)\n    \n    # Generate a walking pattern for 5 steps forward\n    walking_pattern = walker.generate_walking_pattern(\n        num_steps=5, \n        walk_velocity=[0.3, 0.0, 0.0]  # 0.3 m/s forward\n    )\n    \n    print(f\"Generated walking pattern for {len(walking_pattern['step_sequence'])} steps\")\n    print(f\"Total CoM trajectory points: {len(walking_pattern['com_trajectory'])}\")\n    print(f\"Total ZMP trajectory points: {len(walking_pattern['zmp_trajectory'])}\")\n"})}),"\n",(0,t.jsx)(e.h3,{id:"manipulation-control",children:"Manipulation Control"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class ManipulationController:\n    def __init__(self, robot_config):\n        \"\"\"\n        Controller for humanoid manipulation tasks\n        \n        Args:\n            robot_config: Configuration with arm/link parameters\n        \"\"\"\n        self.arm_chain_params = robot_config.get('arm_chain_params', {})\n        self.hand_params = robot_config.get('hand_params', {})\n        self.workspace_limits = robot_config.get('workspace_limits', {})\n        \n        # Initialize inverse kinematics solver\n        self.ik_solver = self._initialize_ik_solver()\n    \n    def _initialize_ik_solver(self):\n        \"\"\"\n        Initialize inverse kinematics solver (using analytical or numerical method)\n        \"\"\"\n        # In a real implementation, this would interface with libraries like\n        # KDL, OpenRAVE, or custom analytical solvers\n        class SimpleIKSolver:\n            def solve(self, target_pose, initial_guess, chain_params):\n                \"\"\"\n                Solve inverse kinematics for the target pose\n                \n                Args:\n                    target_pose: Desired end-effector pose [x, y, z, qw, qx, qy, qz]\n                    initial_guess: Initial joint configuration\n                    chain_params: Link lengths and joint limits\n                \n                Returns:\n                    Joint angles to achieve target pose or None if unreachable\n                \"\"\"\n                # Simplified IK solution - in reality would be more complex\n                # This could use analytical methods for simple chains or\n                # numerical methods (Jacobian pseudoinverse, etc.) for complex ones\n                x, y, z, qw, qx, qy, qz = target_pose\n                \n                # Calculate joint angles (simplified 3DOF arm model)\n                # Shoulder: elevation to height z\n                # Elbow: reach to distance in xy plane\n                # Wrist: orientation\n                \n                # This is a placeholder - real implementation would be much more complex\n                joint_angles = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # 7 DOF arm\n                \n                # Perform actual calculations based on arm geometry\n                dist_xy = np.sqrt(x**2 + y**2)\n                arm_length = chain_params.get('upper_arm', 0.3) + chain_params.get('forearm', 0.3)\n                \n                if dist_xy > arm_length:\n                    # Target out of reach\n                    return None\n                \n                # Calculate shoulder and elbow angles using geometric approach\n                # This is a simplified example, real systems would be more complex\n                elbow_angle = np.arccos(\n                    (chain_params.get('upper_arm', 0.3)**2 + chain_params.get('forearm', 0.3)**2 - dist_xy**2) /\n                    (2 * chain_params.get('upper_arm', 0.3) * chain_params.get('forearm', 0.3))\n                )\n                \n                shoulder_angle = np.arctan2(z, dist_xy) + np.arcsin(\n                    chain_params.get('forearm', 0.3) * np.sin(elbow_angle) / dist_xy\n                )\n                \n                joint_angles[0] = shoulder_angle  # Shoulder elevation\n                joint_angles[1] = 0.0  # Shoulder azimuth - simplified\n                joint_angles[2] = elbow_angle  # Elbow flexion\n                joint_angles[3] = 0.0  # Wrist flexion - simplified\n                joint_angles[4] = 0.0  # Wrist rotation - simplified\n                joint_angles[5] = 0.0  # Wrist abduction - simplified\n                joint_angles[6] = 0.0  # Gripper - simplified\n                \n                return joint_angles\n        \n        return SimpleIKSolver()\n    \n    def compute_manipulation_command(self, task_description):\n        \"\"\"\n        Compute manipulation command based on task description\n        \n        Args:\n            task_description: Dictionary describing the manipulation task\n        \n        Returns:\n            Joint commands for manipulation\n        \"\"\"\n        task_type = task_description.get('type', 'reach')\n        target_pose = task_description.get('target_pose')\n        object_info = task_description.get('object', None)\n        \n        commands = {}\n        \n        if task_type == 'reach':\n            # Simple reaching motion\n            joint_angles = self.ik_solver.solve(\n                target_pose=target_pose,\n                initial_guess=[0, 0, 0, 0, 0, 0, 0],\n                chain_params=self.arm_chain_params\n            )\n            \n            if joint_angles:\n                commands = {\n                    'l_shoulder_pitch': joint_angles[0],\n                    'l_shoulder_yaw': joint_angles[1],\n                    'l_elbow': joint_angles[2],\n                    'l_wrist_flex': joint_angles[3],\n                    'l_wrist_rot': joint_angles[4],\n                    'l_wrist_abd': joint_angles[5],\n                    'l_gripper': joint_angles[6]\n                }\n        \n        elif task_type == 'grasp':\n            # Reaching to grasp position + executing grasp\n            if object_info:\n                grasp_pose = self._calculate_grasp_pose(object_info)\n                joint_angles = self.ik_solver.solve(\n                    target_pose=grasp_pose,\n                    initial_guess=[0, 0, 0, 0, 0, 0, 0],\n                    chain_params=self.arm_chain_params\n                )\n                \n                if joint_angles:\n                    # Move to pre-grasp position first\n                    for i, joint_name in enumerate(['l_shoulder_pitch', 'l_shoulder_yaw', \n                                                   'l_elbow', 'l_wrist_flex', 'l_wrist_rot', \n                                                   'l_wrist_abd']):\n                        commands[joint_name] = joint_angles[i] * 0.8  # 80% to pre-grasp\n                    \n                    # Close gripper when in position\n                    commands['l_gripper'] = 0.0  # Fully closed\n                    \n        elif task_type == 'transport':\n            # Move object from one location to another\n            start_pose = task_description.get('start_pose')\n            end_pose = task_description.get('end_pose')\n            \n            # In a real system, this would involve trajectory planning\n            # to avoid obstacles while maintaining grasp\n            pass  # Implementation would be complex\n        \n        return commands\n    \n    def _calculate_grasp_pose(self, object_info):\n        \"\"\"\n        Calculate appropriate grasp pose for an object\n        \n        Args:\n            object_info: Dictionary with object properties (shape, size, etc.)\n        \n        Returns:\n            Appropriate grasp pose\n        \"\"\"\n        # Calculate approach direction and grasp point based on object properties\n        shape = object_info.get('shape', 'unknown')\n        size = object_info.get('size', [0.1, 0.1, 0.1])  # x, y, z dimensions\n        position = object_info.get('position', [0, 0, 0.8])  # x, y, z position\n        \n        # Default grasp pose (position + orientation)\n        grasp_pose = position.copy()\n        \n        if shape == 'cylinder' or shape == 'can':\n            # Grasp along the cylindrical axis for stability\n            grasp_pose.extend([1, 0, 0, 0])  # [x, y, z, qw, qx, qy, qz] - identity quaternion\n        elif shape == 'rectangular':\n            # Grasp along the shortest dimension for better control\n            min_dim_idx = np.argmin(size)\n            if min_dim_idx == 0:  # x is shortest\n                grasp_pose.extend([0.707, 0.707, 0, 0])  # Rotate 90\xb0 around z\n            elif min_dim_idx == 1:  # y is shortest\n                grasp_pose.extend([0.707, 0, 0.707, 0])  # Rotate 90\xb0 around y\n            else:  # z is shortest\n                grasp_pose.extend([0.707, 0, 0, 0.707])  # Identity (grasp from above)\n        else:\n            # Default approach - grasp from above with palm down\n            grasp_pose.extend([0.707, 0, 0, 0.707])  # Identity quaternion\n        \n        # Adjust approach point to be slightly above the object for safe approach\n        grasp_pose[2] += size[2] / 2 + 0.05  # Half height + 5cm clearance\n        \n        return grasp_pose\n\n# Example usage\nif __name__ == \"__main__\":\n    # Robot configuration\n    robot_config = {\n        'arm_chain_params': {\n            'upper_arm': 0.3,   # Upper arm length (m)\n            'forearm': 0.3,     # Forearm length (m)\n            'shoulder_offset': 0.2,  # Shoulder lateral offset\n        },\n        'workspace_limits': {\n            'min_x': -0.5,\n            'max_x': 0.5,\n            'min_y': -0.3,\n            'max_y': 0.3,\n            'min_z': 0.2,\n            'max_z': 1.0\n        }\n    }\n    \n    manipulator = ManipulationController(robot_config)\n    \n    # Define a task to reach a specific position\n    reach_task = {\n        'type': 'reach',\n        'target_pose': [0.3, 0.2, 0.8, 0.707, 0, 0, 0.707]  # Position + orientation (quaternion)\n    }\n    \n    commands = manipulator.compute_manipulation_command(reach_task)\n    print(f\"Reach commands: {commands}\")\n    \n    # Define a grasping task\n    grasp_task = {\n        'type': 'grasp',\n        'object': {\n            'shape': 'cylinder',\n            'size': [0.05, 0.05, 0.15],  # Diameter 5cm, height 15cm\n            'position': [0.3, 0.0, 0.75]  # On a table 75cm high\n        }\n    }\n    \n    grasp_commands = manipulator.compute_manipulation_command(grasp_task)\n    print(f\"Grasp commands: {grasp_commands}\")\n"})}),"\n",(0,t.jsx)(e.h2,{id:"advanced-topics",children:"Advanced Topics"}),"\n",(0,t.jsx)(e.h3,{id:"whole-body-control",children:"Whole-Body Control"}),"\n",(0,t.jsx)(e.p,{children:"Whole-body control in humanoid robots refers to methodologies that coordinate all available degrees of freedom to achieve multiple tasks simultaneously while respecting physical constraints:"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Task Prioritization"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Primary tasks (e.g., balance, collision avoidance) take precedence"}),"\n",(0,t.jsx)(e.li,{children:"Secondary tasks (e.g., reaching, looking) are fulfilled when possible"}),"\n",(0,t.jsx)(e.li,{children:"Hierarchical optimization manages conflicts"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Constraint Handling"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Joint limits: Physical boundaries of motion"}),"\n",(0,t.jsx)(e.li,{children:"Velocity limits: Prevent damage to actuators"}),"\n",(0,t.jsx)(e.li,{children:"Self-collision avoidance: Prevent links from intersecting"}),"\n",(0,t.jsx)(e.li,{children:"Environmental constraints: Avoid obstacles in workspace"}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class WholeBodyController:\n    def __init__(self, robot_model):\n        """\n        Initialize whole-body controller\n        \n        Args:\n            robot_model: Kinematic and dynamic model of the robot\n        """\n        self.robot_model = robot_model\n        self.task_hierarchy = []  # Ordered list of tasks by priority\n        self.joint_limits = robot_model.get_joint_limits()\n        self.constraints = []\n    \n    def add_task(self, task, priority, weight=1.0):\n        """\n        Add a task to the controller with priority and weight\n        \n        Args:\n            task: Function defining the task (e.g., reach_task, balance_task)\n            priority: Priority level (0 = highest, higher = lower priority)\n            weight: Weight of task in optimization\n        """\n        self.task_hierarchy.append({\n            \'task\': task,\n            \'priority\': priority,\n            \'weight\': weight\n        })\n        \n        # Sort tasks by priority\n        self.task_hierarchy.sort(key=lambda x: x[\'priority\'])\n    \n    def compute_control_command(self, current_state, task_descriptions):\n        """\n        Compute whole-body control command based on tasks and constraints\n        \n        Args:\n            current_state: Current robot state\n            task_descriptions: Descriptions of current tasks to perform\n        \n        Returns:\n            Joint commands for the entire robot\n        """\n        # This is a conceptual implementation\n        # In practice, this would use optimization frameworks like:\n        # - Quadratic Programming (QP)\n        # - Task-Priority Inverse Kinematics\n        # - Model Predictive Control (MPC)\n        \n        # For illustration, here\'s a simplified approach:\n        desired_joints = np.zeros(self.robot_model.dof)  # Degrees of freedom\n        \n        # Process tasks in priority order\n        for task_info in self.task_hierarchy:\n            task = task_info[\'task\']\n            priority = task_info[\'priority\']\n            weight = task_info[\'weight\']\n            \n            # Compute task-specific joint velocities\n            task_joints = self._solve_single_task(task, current_state)\n            \n            # Blend with existing commands based on priority\n            if priority == 0:  # Highest priority - override current\n                desired_joints = task_joints\n            else:\n                # Blend based on priority (lower priority = less influence)\n                blend_factor = 1.0 / (priority + 1)\n                desired_joints = (1 - blend_factor * weight) * desired_joints + \\\n                                blend_factor * weight * task_joints\n        \n        # Apply joint limits\n        desired_joints = np.clip(\n            desired_joints, \n            self.joint_limits[\'lower\'], \n            self.joint_limits[\'upper\']\n        )\n        \n        return desired_joints\n    \n    def _solve_single_task(self, task, state):\n        """\n        Solve a single task (simplified implementation)\n        """\n        # In reality, this would solve a specific optimization problem\n        # for the given task in the current state\n        return np.zeros(self.robot_model.dof)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"learning-based-approaches",children:"Learning-Based Approaches"}),"\n",(0,t.jsx)(e.p,{children:"Modern humanoid robotics increasingly incorporates learning methods to improve performance:"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Reinforcement Learning"}),": Training control policies through trial and error\n",(0,t.jsx)(e.strong,{children:"Imitation Learning"}),": Learning from demonstrations\n",(0,t.jsx)(e.strong,{children:"Adaptation"}),": Adjusting behavior based on experience"]}),"\n",(0,t.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,t.jsx)(e.h3,{id:"balance-problems",children:"Balance Problems"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Instability"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Check CoM calculation accuracy"}),"\n",(0,t.jsx)(e.li,{children:"Verify ZMP tracking"}),"\n",(0,t.jsx)(e.li,{children:"Adjust control gains appropriately"}),"\n",(0,t.jsx)(e.li,{children:"Calibrate sensors if needed"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Falling Forward/Backward"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Adjust ankle stiffness"}),"\n",(0,t.jsx)(e.li,{children:"Modify CoM height control"}),"\n",(0,t.jsx)(e.li,{children:"Check for mechanical imbalances"}),"\n",(0,t.jsx)(e.li,{children:"Verify IMU calibration"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Falling Sideways"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Check hip/ankle roll control"}),"\n",(0,t.jsx)(e.li,{children:"Verify equal weight distribution"}),"\n",(0,t.jsx)(e.li,{children:"Inspect mechanical linkages"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"locomotion-issues",children:"Locomotion Issues"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Drag Feet"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Increase step height"}),"\n",(0,t.jsx)(e.li,{children:"Check ankle actuator limits"}),"\n",(0,t.jsx)(e.li,{children:"Verify timing coordination"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Inconsistent Steps"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Check phase synchronization"}),"\n",(0,t.jsx)(e.li,{children:"Verify sensor feedback"}),"\n",(0,t.jsx)(e.li,{children:"Adjust step duration parameters"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Stumbling"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Improve terrain adaptation"}),"\n",(0,t.jsx)(e.li,{children:"Enhance balance recovery strategies"}),"\n",(0,t.jsx)(e.li,{children:"Verify foot contact detection"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"manipulation-challenges",children:"Manipulation Challenges"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Poor Grasping"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Improve object recognition"}),"\n",(0,t.jsx)(e.li,{children:"Optimize grasp planning"}),"\n",(0,t.jsx)(e.li,{children:"Calibrate hand sensors"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Trajectory Errors"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Adjust joint control gains"}),"\n",(0,t.jsx)(e.li,{children:"Check for mechanical backlash"}),"\n",(0,t.jsx)(e.li,{children:"Verify kinematic calibration"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(e.h3,{id:"design-considerations",children:"Design Considerations"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Redundancy"}),": Build in more DOF than minimally required for task flexibility"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Modularity"}),": Design components for easy replacement and upgrading"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety"}),": Implement multiple safety mechanisms and failsafes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Maintainability"}),": Plan for easy servicing and component replacement"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"control-strategies",children:"Control Strategies"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hierarchical Control"}),": Separate concerns at different levels"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robust Feedback"}),": Implement multiple feedback channels"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptive Control"}),": Adjust parameters based on conditions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Predictive Control"}),": Anticipate future states when possible"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simulation First"}),": Validate algorithms in simulation before hardware"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Progressive Testing"}),": Start with simple movements, increase complexity gradually"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Failure Modes"}),": Test how the robot responds to failures"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Protocols"}),": Always have emergency stops and safe fallbacks"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Kinematic Analysis"}),": Calculate the reachable workspace for a simplified humanoid arm model."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Balance Simulation"}),": Implement a simulation of the inverted pendulum model for humanoid balance."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Gait Planning"}),": Design a simple walking pattern for a basic biped model and verify its stability."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Grasp Planning"}),": Develop an algorithm for determining stable grasp points on simple geometric objects."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Control Architecture"}),": Implement a basic hierarchical controller architecture that separates balance, locomotion, and manipulation tasks."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Humanoid robotics combines mechanical, control, and AI challenges"}),"\n",(0,t.jsx)(e.li,{children:"Balance and locomotion require sophisticated control algorithms"}),"\n",(0,t.jsx)(e.li,{children:"Human-like form factor enables operation in human environments"}),"\n",(0,t.jsx)(e.li,{children:"Safety and robustness are paramount in humanoid design"}),"\n",(0,t.jsx)(e.li,{children:"Learning and adaptation are key for real-world deployment"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:'"Humanoid Robotics: A Reference" by Herr and Bellman'}),"\n",(0,t.jsx)(e.li,{children:'"Introduction to Humanoid Robotics" by Khatib and Park'}),"\n",(0,t.jsx)(e.li,{children:'"Bipedal Robotics" - Technical papers and research'}),"\n",(0,t.jsx)(e.li,{children:'"Whole-Body Control" - Advanced control methods for humanoid robots'}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(e.p,{children:"Continue to Chapter 2: Bipedal Locomotion to explore the mechanics and control strategies for humanoid walking."})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>a});var o=i(6540);const t={},r=o.createContext(t);function s(n){const e=o.useContext(r);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),o.createElement(r.Provider,{value:e},n.children)}}}]);