"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[4374],{4740:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"module-04-isaac-nvidia/part-02-advanced-intelligence/sim-to-real","title":"Sim-to-Real Transfer","description":"This chapter explores the crucial challenge of transferring robotic behaviors learned in simulation to real-world robots. While simulation provides safe, efficient, and cost-effective training environments, the ultimate goal is to deploy these learned behaviors on actual hardware. This chapter will cover the techniques and methodologies that bridge the gap between simulation and reality.","source":"@site/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/03-sim-to-real.md","sourceDirName":"module-04-isaac-nvidia/part-02-advanced-intelligence","slug":"/module-04-isaac-nvidia/part-02-advanced-intelligence/sim-to-real","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/sim-to-real","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba-Akbar956/Physical_AI_And_Humanoid_Robotics_Book/tree/main/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/03-sim-to-real.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Sim-to-Real Transfer"},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/reinforcement-learning"},"next":{"title":"Isaac ROS Integration","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/sim-integration"}}');var i=t(4848),r=t(8453);const s={sidebar_position:3,title:"Sim-to-Real Transfer"},o="Sim-to-Real Transfer",l={},m=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Reality Gap",id:"introduction-to-reality-gap",level:2},{value:"The Sim-to-Real Pipeline",id:"the-sim-to-real-pipeline",level:3},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Implementation of Domain Randomization",id:"implementation-of-domain-randomization",level:3},{value:"Advanced Domain Randomization Techniques",id:"advanced-domain-randomization-techniques",level:3},{value:"System Identification for Reality Gap Reduction",id:"system-identification-for-reality-gap-reduction",level:2},{value:"Parameter Estimation",id:"parameter-estimation",level:3},{value:"Adaptive Control for Sim-to-Real Transfer",id:"adaptive-control-for-sim-to-real-transfer",level:2},{value:"Model Reference Adaptive Control (MRAC)",id:"model-reference-adaptive-control-mrac",level:3},{value:"Validation and Safety Considerations",id:"validation-and-safety-considerations",level:2},{value:"Safety-First Deployment Framework",id:"safety-first-deployment-framework",level:3},{value:"Exercise: Implement Domain Randomization for a Simple Robot",id:"exercise-implement-domain-randomization-for-a-simple-robot",level:2},{value:"Solution",id:"solution",level:3},{value:"Takeaways",id:"takeaways",level:2},{value:"Reading",id:"reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"})}),"\n",(0,i.jsx)(n.p,{children:"This chapter explores the crucial challenge of transferring robotic behaviors learned in simulation to real-world robots. While simulation provides safe, efficient, and cost-effective training environments, the ultimate goal is to deploy these learned behaviors on actual hardware. This chapter will cover the techniques and methodologies that bridge the gap between simulation and reality."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Understand the "reality gap" and its impact on robotic performance'}),"\n",(0,i.jsx)(n.li,{children:"Master domain randomization techniques to improve sim-to-real transfer"}),"\n",(0,i.jsx)(n.li,{children:"Learn about system identification to model real-world discrepancies"}),"\n",(0,i.jsx)(n.li,{children:"Apply adaptive control methods for sim-to-real deployment"}),"\n",(0,i.jsx)(n.li,{children:"Implement validation techniques to ensure safe real-world deployment"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-reality-gap",children:"Introduction to Reality Gap"}),"\n",(0,i.jsx)(n.p,{children:"The reality gap refers to the differences between simulated environments and the real world that can cause behaviors learned in simulation to fail when deployed on actual robots. These differences include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Physical Properties"}),": Inaccuracies in simulation models of friction, elasticity, mass distributions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Noise"}),": Differences in sensor behavior between simulation and reality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actuator Dynamics"}),": Discrepancies in motor response, latency, and control precision"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environmental Factors"}),": Unmodeled external forces, lighting conditions, or air currents"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The sim-to-real transfer problem is fundamental in robotics and requires systematic approaches to ensure that simulation-based learning translates to real-world success."}),"\n",(0,i.jsx)(n.h3,{id:"the-sim-to-real-pipeline",children:"The Sim-to-Real Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Simulation Training \u2192 Domain Randomization \u2192 System Identification \u2192 Real-World Deployment\n"})}),"\n",(0,i.jsx)(n.p,{children:"Each stage of the pipeline addresses specific aspects of the reality gap to improve transfer performance."}),"\n",(0,i.jsx)(n.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,i.jsx)(n.p,{children:"Domain randomization is a key technique that improves sim-to-real transfer by training policies on a wide variety of simulation conditions. Instead of learning from a single, potentially inaccurate, simulation model, the robot learns to adapt to a range of possible environments."}),"\n",(0,i.jsx)(n.h3,{id:"implementation-of-domain-randomization",children:"Implementation of Domain Randomization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom isaacgym import gymapi, gymtorch\nfrom isaacgym import gymutil\n\nclass DomainRandomizationEnv:\n    def __init__(self):\n        # Initialize simulation environment\n        self.gym = gymapi.Gym()\n        self.sim = self.gym.create_sim(0, 0, gymapi.SIM_PHYSX, self._get_sim_params())\n        \n        # Define randomization ranges\n        self.randomization_ranges = {\n            'friction': (0.1, 1.0),           # Range for friction coefficients\n            'mass_multiplier': (0.8, 1.2),   # Range for mass scaling\n            'restitution': (0.0, 0.5),      # Range for bounciness\n            'damping': (0.0, 0.1),          # Range for damping coefficients\n            'stiffness': (0.8, 1.2)         # Range for joint stiffness\n        }\n        \n        # Initialize randomized parameters\n        self.randomized_params = {}\n        \n    def _get_sim_params(self):\n        # Create simulation parameters\n        sim_params = gymapi.SimParams()\n        sim_params.up_axis = gymapi.UP_AXIS_Z\n        sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.81)\n        return sim_params\n        \n    def randomize_environment(self):\n        \"\"\"\n        Apply randomization to environment parameters\n        \"\"\"\n        # Randomize friction\n        friction_range = self.randomization_ranges['friction']\n        self.randomized_params['friction'] = np.random.uniform(\n            friction_range[0], friction_range[1]\n        )\n        \n        # Randomize mass multiplier\n        mass_range = self.randomization_ranges['mass_multiplier']\n        self.randomized_params['mass_multiplier'] = np.random.uniform(\n            mass_range[0], mass_range[1]\n        )\n        \n        # Randomize restitution (bounciness)\n        restitution_range = self.randomization_ranges['restitution']\n        self.randomized_params['restitution'] = np.random.uniform(\n            restitution_range[0], restitution_range[1]\n        )\n        \n        # Randomize damping\n        damping_range = self.randomization_ranges['damping']\n        self.randomized_params['damping'] = np.random.uniform(\n            damping_range[0], damping_range[1]\n        )\n        \n        # Randomize stiffness\n        stiffness_range = self.randomization_ranges['stiffness']\n        self.randomized_params['stiffness'] = np.random.uniform(\n            stiffness_range[0], stiffness_range[1]\n        )\n        \n        print(f\"Randomized parameters: {self.randomized_params}\")\n        \n    def apply_randomization_to_robot(self, robot_asset):\n        \"\"\"\n        Apply randomization parameters to the robot asset\n        \"\"\"\n        # Apply mass randomization\n        mass_multiplier = self.randomized_params['mass_multiplier']\n        \n        # Modify the robot asset with randomized parameters\n        # This is a simplified representation - actual implementation would\n        # modify the asset properties in the simulation\n        print(f\"Applying mass multiplier: {mass_multiplier}\")\n        \n    def reset_environment(self):\n        \"\"\"\n        Reset the environment with new randomization\n        \"\"\"\n        self.randomize_environment()\n        # Reset robot position, velocities, etc.\n        print(\"Environment reset with new randomization\")\n\n# Example usage for sim-to-real transfer\ndef train_with_domain_randomization():\n    env = DomainRandomizationEnv()\n    \n    # Train for multiple episodes with different randomizations\n    for episode in range(1000):  # In practice, this would run for many more episodes\n        if episode % 100 == 0:  # Randomize every 100 episodes\n            env.reset_environment()\n        \n        # Run training step\n        print(f\"Training episode {episode}\")\n        \n    print(\"Training completed with domain randomization\")\n\n# Run the example\nif __name__ == \"__main__\":\n    train_with_domain_randomization()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-domain-randomization-techniques",children:"Advanced Domain Randomization Techniques"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\nfrom collections import deque\n\nclass AdaptiveDomainRandomization:\n    def __init__(self, initial_ranges, adaptation_rate=0.01):\n        """\n        Adaptive domain randomization that modifies randomization parameters\n        based on the performance discrepancy between simulation and reality.\n        \n        Args:\n            initial_ranges: Dict of parameter names to (min, max) tuples\n            adaptation_rate: Rate at which to adjust randomization ranges\n        """\n        self.initial_ranges = initial_ranges.copy()\n        self.current_ranges = initial_ranges.copy()\n        self.adaptation_rate = adaptation_rate\n        \n        # Track performance metrics\n        self.sim_performance = deque(maxlen=100)\n        self.real_performance = deque(maxlen=100)\n        \n    def update_ranges_based_on_performance(self):\n        """\n        Adjust randomization ranges based on sim vs real performance discrepancy\n        """\n        if len(self.sim_performance) < 10 or len(self.real_performance) < 10:\n            return  # Need sufficient data to make comparison\n            \n        # Calculate performance discrepancy\n        avg_sim_perf = sum(self.sim_performance) / len(self.sim_performance)\n        avg_real_perf = sum(self.real_performance) / len(self.real_performance)\n        \n        discrepancy = abs(avg_sim_perf - avg_real_perf)\n        \n        # Adjust ranges proportionally to discrepancy\n        for param_name, (min_val, max_val) in self.current_ranges.items():\n            # Calculate range width\n            current_width = max_val - min_val\n            # Increase range if discrepancy is high\n            new_width = current_width * (1 + discrepancy * self.adaptation_rate)\n            \n            # Calculate new min/max preserving center\n            center = (min_val + max_val) / 2\n            new_min = center - new_width / 2\n            new_max = center + new_width / 2\n            \n            # Update range\n            self.current_ranges[param_name] = (new_min, new_max)\n            \n        print(f"Updated ranges: {self.current_ranges}")\n\n    def randomize_parameter(self, param_name):\n        """\n        Randomize a specific parameter using current ranges\n        """\n        if param_name not in self.current_ranges:\n            raise ValueError(f"Parameter {param_name} not in randomization ranges")\n            \n        min_val, max_val = self.current_ranges[param_name]\n        return np.random.uniform(min_val, max_val)\n\n# Example usage\ndef example_adaptive_domain_randomization():\n    initial_ranges = {\n        \'friction\': (0.1, 1.0),\n        \'mass_multiplier\': (0.8, 1.2),\n        \'restitution\': (0.0, 0.5)\n    }\n    \n    adaptive_dr = AdaptiveDomainRandomization(initial_ranges)\n    \n    # Simulate training iterations\n    for iteration in range(100):\n        # Simulate collecting performance data\n        sim_perf = np.random.normal(0.8, 0.1)  # Simulated performance\n        real_perf = np.random.normal(0.6, 0.1)  # Real performance (lower due to reality gap)\n        \n        adaptive_dr.sim_performance.append(sim_perf)\n        adaptive_dr.real_performance.append(real_perf)\n        \n        # Occasionally update ranges based on performance\n        if iteration % 10 == 0:\n            adaptive_dr.update_ranges_based_on_performance()\n            \n        # Randomize parameters for next iteration\n        friction = adaptive_dr.randomize_parameter(\'friction\')\n        mass_mult = adaptive_dr.randomize_parameter(\'mass_multiplier\')\n        \n        print(f"Iteration {iteration}: friction={friction:.3f}, mass_mult={mass_mult:.3f}")\n\nif __name__ == "__main__":\n    example_adaptive_domain_randomization()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"system-identification-for-reality-gap-reduction",children:"System Identification for Reality Gap Reduction"}),"\n",(0,i.jsx)(n.p,{children:"System identification involves characterizing the actual physical system to better understand and model the differences between simulation and reality."}),"\n",(0,i.jsx)(n.h3,{id:"parameter-estimation",children:"Parameter Estimation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy import signal\n\nclass SystemIdentifier:\n    def __init__(self):\n        # True parameters (unknown in reality, but we'll estimate them)\n        self.true_params = {\n            'mass': 1.0,      # kg\n            'friction': 0.1,  # N*s/m\n            'gravity': 9.81   # m/s^2\n        }\n        \n        # Estimated parameters (what we'll try to find)\n        self.estimated_params = {\n            'mass': 0.8,      # Starting estimate\n            'friction': 0.05, # Starting estimate\n            'gravity': 9.8    # Starting estimate\n        }\n        \n        # Data storage\n        self.inputs = []\n        self.outputs = []\n        \n    def simulate_system(self, params, input_signal, dt=0.01):\n        \"\"\"\n        Simulate a simple system: mass-spring-damper\n        \n        Equation: m*\u1e8d + b*\u1e8b + k*x = F\n        \n        For our case: m*\u1e8d + b*\u1e8b = F - m*g (simplified)\n        \"\"\"\n        time_steps = len(input_signal)\n        positions = np.zeros(time_steps)\n        velocities = np.zeros(time_steps)\n        \n        mass = params['mass']\n        friction = params['friction']\n        gravity = params['gravity']\n        \n        for i in range(1, time_steps):\n            # Calculate acceleration: \u1e8d = (F - b*\u1e8b - m*g) / m\n            acceleration = (input_signal[i] - friction * velocities[i-1] - mass * gravity) / mass\n            # Update velocity: \u1e8b = \u1e8b + \u1e8d*dt\n            velocities[i] = velocities[i-1] + acceleration * dt\n            # Update position: x = x + \u1e8b*dt\n            positions[i] = positions[i-1] + velocities[i] * dt\n            \n        return positions, velocities\n    \n    def collect_data(self, input_signal, dt=0.01):\n        \"\"\"\n        Collect data from the true system (with noise)\n        \"\"\"\n        # Get response from true system\n        true_positions, true_velocities = self.simulate_system(\n            self.true_params, input_signal, dt\n        )\n        \n        # Add some measurement noise to simulate real sensors\n        noise_level = 0.01\n        noisy_positions = true_positions + np.random.normal(0, noise_level, len(true_positions))\n        \n        # Store data\n        self.inputs = np.array(input_signal)\n        self.outputs = np.array(noisy_positions)  # Only position measurements for this example\n        \n        return noisy_positions\n    \n    def cost_function(self, params_array):\n        \"\"\"\n        Cost function for parameter estimation\n        \"\"\"\n        # Convert array back to parameter dict\n        params = {\n            'mass': params_array[0],\n            'friction': params_array[1],\n            'gravity': params_array[2]\n        }\n        \n        # Simulate with these parameters\n        sim_positions, _ = self.simulate_system(params, self.inputs)\n        \n        # Calculate error with measured data\n        error = np.sum((self.outputs - sim_positions)**2)\n        \n        return error\n    \n    def identify_system(self):\n        \"\"\"\n        Identify system parameters using optimization\n        \"\"\"\n        # Initial parameter guess\n        initial_guess = [\n            self.estimated_params['mass'],\n            self.estimated_params['friction'],\n            self.estimated_params['gravity']\n        ]\n        \n        # Bounds for parameters\n        bounds = [\n            (0.1, 5.0),    # mass bounds\n            (0.01, 1.0),   # friction bounds\n            (9.0, 10.0)    # gravity bounds\n        ]\n        \n        # Optimize\n        result = minimize(\n            self.cost_function,\n            initial_guess,\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n        \n        # Update estimated parameters\n        self.estimated_params['mass'] = result.x[0]\n        self.estimated_params['friction'] = result.x[1]\n        self.estimated_params['gravity'] = result.x[2]\n        \n        print(f\"True parameters: {self.true_params}\")\n        print(f\"Estimated parameters: {self.estimated_params}\")\n        print(f\"Optimization success: {result.success}\")\n        print(f\"Final cost: {result.fun}\")\n        \n        return result\n\n# Example usage\ndef example_system_identification():\n    identifier = SystemIdentifier()\n    \n    # Create input signal (e.g., step input or chirp signal)\n    t = np.linspace(0, 10, 1000)  # 10 seconds at 100 Hz\n    input_signal = 10.0 * np.ones_like(t)  # Constant force input\n    \n    # Collect data from the true system (with noise)\n    measured_positions = identifier.collect_data(input_signal)\n    \n    # Identify system parameters\n    result = identifier.identify_system()\n    \n    return identifier\n\nif __name__ == \"__main__\":\n    example_system_identification()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"adaptive-control-for-sim-to-real-transfer",children:"Adaptive Control for Sim-to-Real Transfer"}),"\n",(0,i.jsx)(n.p,{children:"Adaptive control systems can adjust their behavior to compensate for model uncertainties and environmental changes."}),"\n",(0,i.jsx)(n.h3,{id:"model-reference-adaptive-control-mrac",children:"Model Reference Adaptive Control (MRAC)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport matplotlib.pyplot as plt\n\nclass ModelReferenceAdaptiveController:\n    def __init__(self, plant_params, reference_model_params, adaptation_rate=0.1):\n        """\n        Initialize MRAC controller\n        \n        Args:\n            plant_params: Initial estimates of plant parameters [a, b] in \u1e8b = ax + bu\n            reference_model_params: Parameters for reference model [a_m, b_m]\n            adaptation_rate: Rate for parameter adaptation\n        """\n        self.plant_params = np.array(plant_params)  # [\xe2, b\u0302]\n        self.ref_params = np.array(reference_model_params)  # [a_m, b_m]\n        self.adaptation_rate = adaptation_rate\n        \n        # Controller parameters\n        self.k_r = -1.0  # Reference gain\n        self.k_y = -1.0  # Output feedback gain\n        \n        # For adaptation law\n        self.theta = np.zeros(2)  # Parameter adjustments [delta_a, delta_b]\n        \n        # State tracking\n        self.state = 0.0\n        self.ref_state = 0.0\n        self.error = 0.0\n        \n    def control(self, reference_input, actual_output, dt=0.01):\n        """\n        Calculate control input using MRAC\n        """\n        # Calculate reference model state\n        self.ref_state += dt * (\n            self.ref_params[0] * self.ref_state + \n            self.ref_params[1] * reference_input\n        )\n        \n        # Calculate tracking error\n        self.error = self.ref_state - actual_output\n        \n        # Calculate control input\n        control_input = (\n            (self.ref_params[0] - self.plant_params[0]) * actual_output +\n            (self.ref_params[1] - self.plant_params[1]) * self.k_r * reference_input +\n            self.k_r * reference_input - \n            self.k_y * self.error +\n            np.dot(self.theta, [actual_output, reference_input])\n        )\n        \n        # Update adaptive parameters\n        self._update_parameters(actual_output, reference_input, dt)\n        \n        return control_input\n    \n    def _update_parameters(self, y, r, dt):\n        """\n        Update adaptive parameters using MIT rule\n        """\n        # Gradient of error with respect to parameters\n        phi = np.array([y, self.k_r * r])\n        \n        # Update theta using MIT rule\n        self.theta += self.adaptation_rate * self.error * phi * dt\n\nclass AdaptiveRoboticSystem:\n    def __init__(self):\n        # True plant parameters (unknown to controller)\n        self.true_plant_params = np.array([0.5, 1.2])  # [a, b] in \u1e8b = ax + bu\n        self.state = 0.0\n        \n        # Initialize MRAC with initial estimates\n        initial_estimates = [0.3, 1.0]  # Initial estimates\n        reference_params = [-2.0, 1.0]  # Stable reference model\n        \n        self.mrac = ModelReferenceAdaptiveController(\n            initial_estimates, reference_params\n        )\n    \n    def step(self, reference_input, dt=0.01):\n        """\n        Step the adaptive system\n        """\n        # Get control input from MRAC\n        control_input = self.mrac.control(reference_input, self.state, dt)\n        \n        # Apply control to actual plant (with true parameters)\n        state_dot = (\n            self.true_plant_params[0] * self.state + \n            self.true_plant_params[1] * control_input\n        )\n        self.state += dt * state_dot\n        \n        return self.state\n\ndef simulate_adaptive_control():\n    """\n    Simulate adaptive control system\n    """\n    adaptive_sys = AdaptiveRoboticSystem()\n    \n    # Simulation parameters\n    dt = 0.01\n    t_max = 10.0\n    t = np.arange(0, t_max, dt)\n    \n    # Reference input (step command)\n    reference = np.ones_like(t) * 1.0\n    reference[int(0.5/dt):] = 2.0  # Step to 2.0 after 0.5 seconds\n    \n    # Arrays to store results\n    states = []\n    errors = []\n    \n    # Simulate\n    for i in range(len(t)):\n        state = adaptive_sys.step(reference[i], dt)\n        states.append(state)\n        errors.append(adaptive_sys.mrac.error)\n    \n    # Plot results\n    plt.figure(figsize=(12, 8))\n    \n    plt.subplot(2, 1, 1)\n    plt.plot(t, reference, \'r--\', label=\'Reference\', linewidth=2)\n    plt.plot(t, states, \'b-\', label=\'System Output\', linewidth=2)\n    plt.title(\'Adaptive Control Response\')\n    plt.xlabel(\'Time [s]\')\n    plt.ylabel(\'State\')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.subplot(2, 1, 2)\n    plt.plot(t, errors, \'g-\', label=\'Tracking Error\', linewidth=2)\n    plt.title(\'Tracking Error\')\n    plt.xlabel(\'Time [s]\')\n    plt.ylabel(\'Error\')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return t, reference, states, errors\n\n# Example usage\nif __name__ == "__main__":\n    results = simulate_adaptive_control()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"validation-and-safety-considerations",children:"Validation and Safety Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Before deploying simulation-trained behaviors on real robots, thorough validation is essential."}),"\n",(0,i.jsx)(n.h3,{id:"safety-first-deployment-framework",children:"Safety-First Deployment Framework"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom enum import Enum\n\nclass SafetyLevel(Enum):\n    OFF = 0\n    MONITORING = 1\n    LIMITED = 2\n    FULL = 3\n\nclass SafetyValidator:\n    def __init__(self, safety_level=SafetyLevel.MONITORING):\n        self.safety_level = safety_level\n        self.safety_limits = {\n            \'position\': 5.0,    # meters\n            \'velocity\': 2.0,    # m/s\n            \'acceleration\': 5.0, # m/s^2\n            \'torque\': 100.0     # N-m for joints\n        }\n        self.emergency_stop = False\n        \n    def validate_action(self, action, current_state):\n        """\n        Validate an action before execution based on safety constraints\n        """\n        if self.emergency_stop:\n            return False, "Emergency stop active"\n        \n        # Check if we\'re in a safe state\n        is_safe_state, state_desc = self._check_safe_state(current_state)\n        if not is_safe_state:\n            return False, f"Unsafe state: {state_desc}"\n        \n        # Project the next state based on action\n        projected_state = self._project_state(current_state, action)\n        \n        # Check if projected state is safe\n        is_safe_action, action_desc = self._check_safe_action(projected_state)\n        if not is_safe_action:\n            return False, f"Unsafe action: {action_desc}"\n        \n        return True, "Action validated"\n    \n    def _check_safe_state(self, state):\n        """\n        Check if current state is safe for continued operation\n        """\n        # Check position limits\n        if np.any(np.abs(state[\'position\']) > self.safety_limits[\'position\']):\n            return False, "Position limit exceeded"\n        \n        # Check velocity limits\n        if np.any(np.abs(state[\'velocity\']) > self.safety_limits[\'velocity\']):\n            return False, "Velocity limit exceeded"\n        \n        # More checks can be added as needed\n        return True, "State is safe"\n    \n    def _project_state(self, current_state, action, dt=0.01):\n        """\n        Project the next state based on action (simplified dynamics)\n        """\n        # This is a simplified state projection\n        # In practice, this would use more detailed dynamics model\n        projected_state = current_state.copy()\n        \n        # Update positions based on velocities\n        projected_state[\'position\'] += current_state[\'velocity\'] * dt\n        \n        # Update velocities based on accelerations (estimated from action)\n        estimated_acceleration = action * 0.1  # Simplified relationship\n        projected_state[\'velocity\'] += estimated_acceleration * dt\n        \n        return projected_state\n    \n    def _check_safe_action(self, projected_state):\n        """\n        Check if projected state is within safety limits\n        """\n        # Check position limits\n        if np.any(np.abs(projected_state[\'position\']) > self.safety_limits[\'position\']):\n            return False, "Projected position limit exceeded"\n        \n        # Check velocity limits\n        if np.any(np.abs(projected_state[\'velocity\']) > self.safety_limits[\'velocity\']):\n            return False, "Projected velocity limit exceeded"\n        \n        # More checks can be added as needed\n        return True, "Action is safe"\n    \n    def trigger_emergency_stop(self, reason="Safety limit violation"):\n        """\n        Trigger emergency stop\n        """\n        print(f"EMERGENCY STOP: {reason}")\n        self.emergency_stop = True\n        \n    def reset_emergency_stop(self):\n        """\n        Reset emergency stop\n        """\n        self.emergency_stop = False\n        print("Emergency stop reset")\n\nclass SimToRealDeploymentManager:\n    def __init__(self):\n        self.safety_validator = SafetyValidator(SafetyLevel.FULL)\n        self.confidence_threshold = 0.7  # Minimum confidence for action execution\n        self.performance_threshold = 0.6  # Minimum performance for continued operation\n        \n    def deploy_with_validation(self, policy, state_estimator, robot_interface):\n        """\n        Deploy policy with safety and validation checks\n        """\n        print("Starting sim-to-real deployment with validation...")\n        \n        while True:\n            # Get current state from robot\n            current_state = state_estimator.get_state()\n            \n            # Get action from policy\n            action, confidence = policy.get_action(current_state)\n            \n            # Validate action safety\n            is_safe, reason = self.safety_validator.validate_action(action, current_state)\n            \n            if is_safe and confidence > self.confidence_threshold:\n                # Execute action\n                robot_interface.execute_action(action)\n                print(f"Action executed with confidence: {confidence:.3f}")\n            else:\n                if not is_safe:\n                    print(f"Action rejected due to safety: {reason}")\n                    self.safety_validator.trigger_emergency_stop(reason)\n                else:\n                    print(f"Action rejected due to low confidence: {confidence:.3f}")\n                \n                # Execute safe action instead (e.g., stop)\n                robot_interface.execute_safe_action()\n            \n            # Check overall system performance\n            performance = robot_interface.get_performance_metric()\n            if performance < self.performance_threshold:\n                print(f"Performance below threshold: {performance:.3f}")\n                self.safety_validator.trigger_emergency_stop("Low performance")\n                break\n                \n            # Check for termination conditions\n            if robot_interface.is_task_complete():\n                print("Task completed successfully")\n                break\n\n# Example usage\ndef example_deployment():\n    """\n    Example of sim-to-real deployment with safety validation\n    """\n    class MockPolicy:\n        def get_action(self, state):\n            # Return a random action with some confidence\n            action = np.random.uniform(-1, 1, size=state[\'position\'].shape)\n            confidence = np.random.uniform(0.5, 1.0)\n            return action, confidence\n    \n    class MockStateEstimator:\n        def get_state(self):\n            return {\n                \'position\': np.random.uniform(-1, 1, size=3),\n                \'velocity\': np.random.uniform(-0.5, 0.5, size=3)\n            }\n    \n    class MockRobotInterface:\n        def execute_action(self, action):\n            print(f"Executing action: {action[:2]}...")  # Show first 2 elements\n            \n        def execute_safe_action(self):\n            print("Executing safe action (stop)")\n            \n        def get_performance_metric(self):\n            return np.random.uniform(0.4, 1.0)\n            \n        def is_task_complete(self):\n            # Randomly terminate after some time\n            return np.random.rand() < 0.02  # ~2% chance per step\n    \n    # Create deployment manager\n    deployment_manager = SimToRealDeploymentManager()\n    \n    # Create mock components\n    policy = MockPolicy()\n    state_estimator = MockStateEstimator()\n    robot_interface = MockRobotInterface()\n    \n    # Deploy with validation\n    deployment_manager.deploy_with_validation(policy, state_estimator, robot_interface)\n\nif __name__ == "__main__":\n    example_deployment()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"exercise-implement-domain-randomization-for-a-simple-robot",children:"Exercise: Implement Domain Randomization for a Simple Robot"}),"\n",(0,i.jsx)(n.p,{children:"Create a simulation environment with domain randomization for a simple wheeled robot that needs to navigate to a target. Implement the randomization of friction, motor dynamics, and sensor noise, then train a policy that can adapt to these variations."}),"\n",(0,i.jsx)(n.h3,{id:"solution",children:"Solution"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport random\n\nclass SimpleWheeledRobotEnv:\n    def __init__(self):\n        # Robot state\n        self.position = np.array([0.0, 0.0])  # x, y\n        self.velocity = np.array([0.0, 0.0])  # dx, dy\n        self.target = np.array([5.0, 5.0])    # Target position\n        \n        # Randomized parameters\n        self.params = {\n            \'friction_coeff\': 0.1,\n            \'motor_constant\': 1.0,\n            \'sensor_noise_std\': 0.01\n        }\n        \n        # Action space: left_wheel_force, right_wheel_force\n        self.action_space = 2\n        # Observation space: x, y, dx, dy, target_x, target_y\n        self.observation_space = 6\n        \n    def reset(self):\n        """Reset environment with new randomization"""\n        self.position = np.array([0.0, 0.0])\n        self.velocity = np.array([0.0, 0.0])\n        \n        # Randomize parameters for this episode\n        self.params[\'friction_coeff\'] = random.uniform(0.05, 0.3)\n        self.params[\'motor_constant\'] = random.uniform(0.8, 1.2)\n        self.params[\'sensor_noise_std\'] = random.uniform(0.005, 0.02)\n        \n        return self._get_observation()\n    \n    def _get_observation(self):\n        """Get observation with noise"""\n        noise = np.random.normal(0, self.params[\'sensor_noise_std\'], size=6)\n        obs = np.concatenate([\n            self.position,\n            self.velocity,\n            self.target\n        ])\n        return obs + noise\n    \n    def step(self, action):\n        """Execute action and return (observation, reward, done, info)"""\n        # Simplified 2-wheel robot dynamics\n        left_force, right_force = action\n        linear_force = (left_force + right_force) / 2\n        angular_force = (right_force - left_force) / 2\n        \n        # Convert forces to x, y forces (simplified for this example)\n        direction = np.arctan2(self.velocity[1], self.velocity[0])\n        if np.linalg.norm(self.velocity) < 0.01:  # If not moving, use last direction\n            direction = 0.0  # Default direction\n            \n        fx = linear_force * np.cos(direction) - angular_force * np.sin(direction)\n        fy = linear_force * np.sin(direction) + angular_force * np.cos(direction)\n        \n        # Apply friction\n        friction_force = -self.params[\'friction_coeff\'] * self.velocity\n        \n        # Calculate acceleration (F = ma, assuming m=1)\n        acceleration = np.array([fx, fy]) + friction_force\n        \n        # Update velocity and position\n        dt = 0.1  # Time step\n        self.velocity += acceleration * dt\n        self.position += self.velocity * dt\n        \n        # Calculate reward (negative distance to target)\n        distance_to_target = np.linalg.norm(self.position - self.target)\n        reward = -distance_to_target\n        \n        # Check if done (reached target or too far)\n        done = distance_to_target < 0.5 or np.linalg.norm(self.position) > 10.0\n        \n        return self._get_observation(), reward, done, {}\n    \n    def get_randomization_info(self):\n        """Get current randomization parameters"""\n        return self.params\n\ndef simple_training_loop():\n    """Simple training loop with domain randomization"""\n    env = SimpleWheeledRobotEnv()\n    episodes = 1000\n    \n    for episode in range(episodes):\n        obs = env.reset()\n        total_reward = 0\n        steps = 0\n        \n        # Simple random policy for this example\n        while steps < 100:\n            # Generate random action (in a real implementation, \n            # this would come from a trained policy)\n            action = np.random.uniform(-1.0, 1.0, size=2)\n            \n            obs, reward, done, info = env.step(action)\n            total_reward += reward\n            steps += 1\n            \n            if done:\n                break\n        \n        # Print progress every 100 episodes\n        if episode % 100 == 0:\n            randomization_info = env.get_randomization_info()\n            print(f"Episode {episode}: Total reward = {total_reward:.2f}")\n            print(f"  Randomization: {randomization_info}")\n    \n    print("Training completed!")\n\nif __name__ == "__main__":\n    simple_training_loop()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"takeaways",children:"Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The reality gap is a fundamental challenge in robotics that requires systematic approaches to address"}),"\n",(0,i.jsx)(n.li,{children:"Domain randomization is a powerful technique that exposes the robot to a wide variety of conditions during training"}),"\n",(0,i.jsx)(n.li,{children:"System identification helps quantify differences between simulation and reality"}),"\n",(0,i.jsx)(n.li,{children:"Adaptive control systems can adjust their behavior to compensate for uncertainties"}),"\n",(0,i.jsx)(n.li,{children:"Safety validation is essential before deploying simulation-trained behaviors on real robots"}),"\n",(0,i.jsx)(n.li,{children:"Combining multiple techniques often yields better sim-to-real transfer than relying on a single approach"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"reading",children:"Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World" by Tobin et al.'}),"\n",(0,i.jsx)(n.li,{children:'"Sim-to-Real Transfer of Robotic Control with Dynamics Randomization" by Peng et al.'}),"\n",(0,i.jsx)(n.li,{children:'"System Identification: Theory for the User" by Ljung'}),"\n",(0,i.jsx)(n.li,{children:"NVIDIA Isaac Lab Documentation on Domain Randomization"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"Continue to Chapter 4: Isaac Simulation Overview to learn about the technical implementation of simulation environments in the Isaac ecosystem."})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var a=t(6540);const i={},r=a.createContext(i);function s(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);