"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[4986],{8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var a=s(6540);const t={},r=a.createContext(t);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(r.Provider,{value:n},e.children)}},9134:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-04-isaac-nvidia/part-02-advanced-intelligence/sim-integration","title":"Isaac ROS Integration","description":"This chapter explores the deeper integration patterns between NVIDIA Isaac and ROS (Robot Operating System), focusing on advanced integration techniques that leverage both NVIDIA\'s GPU-accelerated computing and the extensive ROS ecosystem. This chapter builds on the foundational concepts from the previous chapter and explores more complex integration scenarios.","source":"@site/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/03-sim-integration.md","sourceDirName":"module-04-isaac-nvidia/part-02-advanced-intelligence","slug":"/module-04-isaac-nvidia/part-02-advanced-intelligence/sim-integration","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/sim-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba-Akbar956/Physical_AI_And_Humanoid_Robotics_Book/tree/main/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/03-sim-integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Isaac ROS Integration"},"sidebar":"tutorialSidebar","previous":{"title":"Sim-to-Real Transfer","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-04-isaac-nvidia/part-02-advanced-intelligence/sim-to-real"},"next":{"title":"Humanoid Kinematics","permalink":"/Physical_AI_And_Humanoid_Robotics_Book/docs/module-05-humanoid-control/part-01-locomotion/humanoid-kinematics"}}');var t=s(4848),r=s(8453);const i={sidebar_position:4,title:"Isaac ROS Integration"},o="Isaac ROS Integration",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction: Advanced Integration Patterns",id:"introduction-advanced-integration-patterns",level:2},{value:"Integration Architecture Patterns",id:"integration-architecture-patterns",level:3},{value:"Performance Considerations",id:"performance-considerations",level:3},{value:"Advanced Integration Techniques",id:"advanced-integration-techniques",level:2},{value:"Data Flow Optimization",id:"data-flow-optimization",level:3},{value:"Component Synchronization",id:"component-synchronization",level:3},{value:"GPU Memory Management",id:"gpu-memory-management",level:3},{value:"Practical Implementation",id:"practical-implementation",level:2},{value:"Advanced Isaac ROS Node",id:"advanced-isaac-ros-node",level:3},{value:"Custom Isaac ROS Integration",id:"custom-isaac-ros-integration",level:3},{value:"Isaac Sim Integration",id:"isaac-sim-integration",level:2},{value:"Isaac Sim to ROS Bridge",id:"isaac-sim-to-ros-bridge",level:3},{value:"Performance Optimization Techniques",id:"performance-optimization-techniques",level:2},{value:"GPU Memory Management",id:"gpu-memory-management-1",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Hardware Acceleration Problems",id:"hardware-acceleration-problems",level:3},{value:"Integration Challenges",id:"integration-challenges",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Architecture Design",id:"architecture-design",level:3},{value:"Development Workflow",id:"development-workflow",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"isaac-ros-integration",children:"Isaac ROS Integration"})}),"\n",(0,t.jsx)(n.p,{children:"This chapter explores the deeper integration patterns between NVIDIA Isaac and ROS (Robot Operating System), focusing on advanced integration techniques that leverage both NVIDIA's GPU-accelerated computing and the extensive ROS ecosystem. This chapter builds on the foundational concepts from the previous chapter and explores more complex integration scenarios."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Design advanced architectures for Isaac-ROS integration"}),"\n",(0,t.jsx)(n.li,{children:"Implement GPU-accelerated perception pipelines with ROS interfaces"}),"\n",(0,t.jsx)(n.li,{children:"Create custom Isaac ROS nodes for specialized applications"}),"\n",(0,t.jsx)(n.li,{children:"Optimize data flow between Isaac components and ROS systems"}),"\n",(0,t.jsx)(n.li,{children:"Build fault-tolerant Isaac-ROS integration systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-advanced-integration-patterns",children:"Introduction: Advanced Integration Patterns"}),"\n",(0,t.jsx)(n.p,{children:"Building on the foundational Isaac ROS integration concepts, this chapter explores advanced patterns that enable complex robotics applications. Advanced integration involves not just connecting Isaac components to ROS, but creating seamless workflows that leverage the strengths of both platforms."}),"\n",(0,t.jsx)(n.p,{children:"Key advanced integration concepts include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Component Orchestration"}),": Coordinating multiple Isaac and ROS components for complex tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Pipeline Optimization"}),": Efficient data flow between GPU-accelerated and traditional processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Performance"}),": Achieving low-latency processing across the integrated system"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fault Tolerance"}),": Graceful degradation when components fail"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"integration-architecture-patterns",children:"Integration Architecture Patterns"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pipeline Integration"}),": Sequencing Isaac and ROS components into cohesive processing chains\n",(0,t.jsx)(n.strong,{children:"Parallel Processing"}),": Utilizing both GPU and CPU resources simultaneously\n",(0,t.jsx)(n.strong,{children:"Feedback Loops"}),": Creating control systems that utilize Isaac perception with ROS control\n",(0,t.jsx)(n.strong,{children:"Resource Management"}),": Efficient allocation of GPU and CPU resources"]}),"\n",(0,t.jsx)(n.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory Management"}),": Minimizing data transfers between CPU and GPU\n",(0,t.jsx)(n.strong,{children:"Processing Pipelines"}),": Designing for maximum throughput with minimum latency\n",(0,t.jsx)(n.strong,{children:"Synchronization"}),": Coordinating multi-threaded and GPU-accelerated operations\n",(0,t.jsx)(n.strong,{children:"Resource Utilization"}),": Balancing GPU and CPU workload efficiently"]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-integration-techniques",children:"Advanced Integration Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"data-flow-optimization",children:"Data Flow Optimization"}),"\n",(0,t.jsx)(n.p,{children:"Efficient data flow is crucial for maximizing the benefit of GPU acceleration in robotics applications:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Input Data \u2192 Memory Pool \u2192 GPU Processing \u2192 Optimized Output\n                \u2191              \u2193\n        Minimize Transfers  Maximize Reuse\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory Pooling"}),": Pre-allocating GPU memory for repeated operations\n",(0,t.jsx)(n.strong,{children:"Zero-Copy Transfer"}),": Using unified memory where possible\n",(0,t.jsx)(n.strong,{children:"Batch Processing"}),": Grouping operations to maximize GPU utilization\n",(0,t.jsx)(n.strong,{children:"Stream Processing"}),": Using CUDA streams to overlap computation and transfer"]}),"\n",(0,t.jsx)(n.h3,{id:"component-synchronization",children:"Component Synchronization"}),"\n",(0,t.jsx)(n.p,{children:"Synchronizing Isaac and ROS components requires careful attention to timing and data consistency:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import threading\nimport queue\nimport time\nfrom collections import deque\n\nclass IsaacROSSyncManager:\n    def __init__(self, max_sync_delay=0.1):\n        \"\"\"\n        Manage synchronization between Isaac and ROS components\n        \n        Args:\n            max_sync_delay: Maximum acceptable delay between synced messages (seconds)\n        \"\"\"\n        self.max_sync_delay = max_sync_delay\n        self.sync_queues = {}  # Topic-specific queues\n        self.timestamps = {}   # Track message timestamps\n        self.sync_lock = threading.Lock()\n        \n        # Statistics for performance monitoring\n        self.stats = {\n            'sync_success_rate': 0,\n            'average_delay': 0,\n            'dropped_messages': 0,\n            'total_messages': 0\n        }\n    \n    def register_topic_pair(self, topic1, topic2, sync_callback):\n        \"\"\"\n        Register a pair of topics to keep synchronized\n        \n        Args:\n            topic1: Name of first topic\n            topic2: Name of second topic\n            sync_callback: Function to call when messages are synced\n        \"\"\"\n        key = (topic1, topic2)\n        self.sync_queues[key] = {\n            'queue1': queue.Queue(),\n            'queue2': queue.Queue(),\n            'callback': sync_callback\n        }\n    \n    def add_message(self, topic, data):\n        \"\"\"\n        Add a message from a topic to the synchronization system\n        \"\"\"\n        with self.sync_lock:\n            self.stats['total_messages'] += 1\n            \n            # Find which topic pair this belongs to\n            for (t1, t2), queues in self.sync_queues.items():\n                if topic == t1:\n                    queues['queue1'].put((time.time(), data))\n                    self._attempt_sync((t1, t2), queues)\n                    break\n                elif topic == t2:\n                    queues['queue2'].put((time.time(), data))\n                    self._attempt_sync((t1, t2), queues)\n                    break\n    \n    def _attempt_sync(self, topic_pair, queues):\n        \"\"\"\n        Attempt to sync messages from both queues\n        \"\"\"\n        q1, q2 = queues['queue1'], queues['queue2']\n        \n        if not q1.empty() and not q2.empty():\n            # Peek at the earliest messages\n            t1, data1 = q1.queue[0]\n            t2, data2 = q2.queue[0]\n            \n            # Check if timestamps are within acceptable sync window\n            time_diff = abs(t1 - t2)\n            \n            if time_diff <= self.max_sync_delay:\n                # Sync successful - remove messages and call callback\n                q1.get()  # Remove from queue\n                q2.get()  # Remove from queue\n                \n                # Update stats\n                self.stats['sync_success_rate'] = min(\n                    1.0, \n                    self.stats['sync_success_rate'] + 0.01\n                )\n                self.stats['average_delay'] = (\n                    self.stats['average_delay'] * 0.9 + time_diff * 0.1\n                )\n                \n                # Call synchronization callback\n                callback = queues['callback']\n                callback(data1, data2, time_diff)\n            else:\n                # Messages too far apart - check if we should drop older ones\n                self._cleanup_old_messages(topic_pair, queues)\n    \n    def _cleanup_old_messages(self, topic_pair, queues):\n        \"\"\"\n        Remove old messages that will never find a sync partner\n        \"\"\"\n        current_time = time.time()\n        \n        # Check queue 1\n        while not queues['queue1'].empty():\n            timestamp, data = queues['queue1'].queue[0]\n            if current_time - timestamp > self.max_sync_delay * 2:\n                queues['queue1'].get()  # Drop message\n                self.stats['dropped_messages'] += 1\n            else:\n                break\n        \n        # Check queue 2\n        while not queues['queue2'].empty():\n            timestamp, data = queues['queue2'].queue[0]\n            if current_time - timestamp > self.max_sync_delay * 2:\n                queues['queue2'].get()  # Drop message\n                self.stats['dropped_messages'] += 1\n            else:\n                break\n\n# Example usage\ndef perception_control_callback(perception_data, control_data, sync_delay):\n    print(f\"Synced perception and control data, delay: {sync_delay:.3f}s\")\n    # Process synchronized data...\n\n# Initialize sync manager\nsync_manager = IsaacROSSyncManager(max_sync_delay=0.05)  # 50ms sync window\nsync_manager.register_topic_pair(\n    '/cuda_perception/output', \n    '/ros_control/input', \n    perception_control_callback\n)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"gpu-memory-management",children:"GPU Memory Management"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport gc\n\nclass GPUMemoryManager:\n    def __init__(self, device):\n        """\n        Manage GPU memory for Isaac ROS integration\n        \n        Args:\n            device: CUDA device to manage\n        """\n        self.device = device\n        self.memory_pool = {}  # Pre-allocated tensors\n        self.access_log = {}   # Track tensor usage\n        self.max_memory = torch.cuda.get_device_properties(device).total_memory\n        self.utilization_threshold = 0.85  # Start optimization at 85% usage\n    \n    def allocate_tensor(self, shape, dtype=torch.float32, persistent=False, name=None):\n        """\n        Allocate a tensor, with option for persistent allocation\n        \n        Args:\n            shape: Shape of the tensor\n            dtype: Data type\n            persistent: Whether to keep in memory pool after use\n            name: Optional name for tracking\n        """\n        current_memory = torch.cuda.memory_allocated(self.device)\n        current_utilization = current_memory / self.max_memory\n        \n        # Check if we need to clean up\n        if current_utilization > self.utilization_threshold:\n            self.cleanup_memory()\n        \n        # Check if we have this tensor shape in our pool\n        if persistent and shape in self.memory_pool:\n            return self.memory_pool[shape]\n        \n        # Allocate new tensor\n        tensor = torch.zeros(shape, dtype=dtype, device=self.device)\n        \n        # Add to pool if persistent\n        if persistent:\n            self.memory_pool[shape] = tensor\n            self.access_log[shape] = {\'last_access\': time.time(), \'access_count\': 1}\n        \n        return tensor\n    \n    def get_cached_tensor(self, shape, dtype=torch.float32, name=None):\n        """\n        Get a cached tensor, allocating if not available\n        """\n        if shape in self.memory_pool:\n            tensor = self.memory_pool[shape]\n            # Update access log\n            if shape in self.access_log:\n                self.access_log[shape][\'last_access\'] = time.time()\n                self.access_log[shape][\'access_count\'] += 1\n            return tensor\n        else:\n            return self.allocate_tensor(shape, dtype, persistent=True, name=name)\n    \n    def cleanup_memory(self):\n        """\n        Clean up GPU memory by removing unused tensors\n        """\n        current_time = time.time()\n        \n        # Remove tensors that haven\'t been accessed recently\n        tensors_to_remove = []\n        for shape, tensor in self.memory_pool.items():\n            if shape in self.access_log:\n                last_access = self.access_log[shape][\'last_access\']\n                # Remove tensors not accessed in last 10 seconds\n                if current_time - last_access > 10.0:\n                    tensors_to_remove.append(shape)\n        \n        for shape in tensors_to_remove:\n            del self.memory_pool[shape]\n            if shape in self.access_log:\n                del self.access_log[shape]\n        \n        # Run garbage collection\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    def get_status(self):\n        """\n        Get current GPU memory status\n        """\n        allocated = torch.cuda.memory_allocated(self.device)\n        reserved = torch.cuda.memory_reserved(self.device)\n        \n        return {\n            \'allocated_memory\': allocated,\n            \'reserved_memory\': reserved,\n            \'max_memory\': self.max_memory,\n            \'utilization\': allocated / self.max_memory,\n            \'pool_size\': len(self.memory_pool)\n        }\n'})}),"\n",(0,t.jsx)(n.h2,{id:"practical-implementation",children:"Practical Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"advanced-isaac-ros-node",children:"Advanced Isaac ROS Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, PointCloud2\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Float32\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom collections import defaultdict, deque\nimport threading\n\nclass AdvancedIsaacROSNode(Node):\n    def __init__(self):\n        super().__init__('advanced_isaac_ros_node')\n        \n        # Initialize parameters\n        self.declare_parameter('model_path', '/models/default.pt')\n        self.declare_parameter('confidence_threshold', 0.5)\n        self.declare_parameter('max_objects', 10)\n        self.declare_parameter('processing_rate', 10.0)  # Hz\n        self.declare_parameter('use_gpu', True)\n        self.declare_parameter('gpu_device_id', 0)\n        \n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )\n        self.vel_cmd_pub = self.create_publisher(\n            Twist, '/cmd_vel', 10\n        )\n        self.perf_pub = self.create_publisher(\n            Float32, '/isaac_ros_performance', 10\n        )\n        \n        # Initialize GPU memory manager\n        gpu_id = self.get_parameter('gpu_device_id').value\n        if self.get_parameter('use_gpu').value and torch.cuda.is_available():\n            self.gpu_available = True\n            self.device = torch.device(f'cuda:{gpu_id}')\n            self.memory_manager = GPUMemoryManager(self.device)\n            self.get_logger().info(f'Using GPU: cuda:{gpu_id}')\n        else:\n            self.gpu_available = False\n            self.device = torch.device('cpu')\n            self.get_logger().info('Using CPU for processing')\n        \n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n        \n        # Processing pipeline components\n        self.model = self._load_model()\n        self.preprocessor = self._setup_preprocessor()\n        self.postprocessor = self._setup_postprocessor()\n        \n        # Data buffers and synchronization\n        self.image_buffer = deque(maxlen=10)  # Buffer for processing\n        self.imu_buffer = deque(maxlen=10)    # Buffer for sensor fusion\n        self.processing_lock = threading.Lock()\n        \n        # Performance tracking\n        self.processing_times = deque(maxlen=100)\n        self.frame_count = 0\n        self.last_process_time = time.time()\n        \n        # Spinning timer for processing\n        self.process_timer = self.create_timer(\n            1.0 / self.get_parameter('processing_rate').value,\n            self.process_callback\n        )\n        \n        self.get_logger().info('Advanced Isaac ROS Node initialized')\n\n    def _load_model(self):\n        \"\"\"\n        Load the deep learning model with GPU optimization\n        \"\"\"\n        try:\n            if self.gpu_available:\n                # Load model to GPU and optimize\n                model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n                \n                # Optimize model with TensorRT if available\n                try:\n                    import tensorrt as trt\n                    # In a real implementation, you would convert to TensorRT\n                    # For now, just move to GPU\n                    model = model.to(self.device)\n                    self.get_logger().info('Model loaded and optimized for GPU')\n                except ImportError:\n                    # Use regular GPU acceleration\n                    model = model.to(self.device)\n                    self.get_logger().info('Model loaded to GPU (TensorRT unavailable)')\n            else:\n                # Load model to CPU\n                model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n                self.get_logger().info('Model loaded to CPU')\n            \n            model.eval()  # Set to evaluation mode\n            return model\n            \n        except Exception as e:\n            self.get_logger().error(f'Error loading model: {e}')\n            # Return a placeholder model for simulation\n            return self._create_placeholder_model()\n\n    def _create_placeholder_model(self):\n        \"\"\"\n        Create a placeholder model if the real one fails to load\n        \"\"\"\n        class PlaceholderModel(nn.Module):\n            def __init__(self):\n                super().__init__()\n            \n            def forward(self, x):\n                # Return mock detections\n                batch_size = x.shape[0]\n                mock_detections = []\n                \n                for i in range(batch_size):\n                    # Simulate finding 2-3 objects\n                    num_objects = np.random.randint(2, 4)\n                    detections = []\n                    \n                    for j in range(num_objects):\n                        det = {\n                            'bbox': [np.random.randint(0, 640), \n                                    np.random.randint(0, 480), \n                                    50, 50],  # [x, y, width, height]\n                            'conf': np.random.uniform(0.7, 0.95),\n                            'class': np.random.choice(['person', 'car', 'bicycle'])\n                        }\n                        detections.append(det)\n                    \n                    mock_detections.append(detections)\n                \n                return mock_detections\n        \n        return PlaceholderModel()\n\n    def _setup_preprocessor(self):\n        \"\"\"\n        Setup image preprocessing pipeline\n        \"\"\"\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((640, 640)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def _setup_postprocessor(self):\n        \"\"\"\n        Setup post-processing pipeline\n        \"\"\"\n        def postprocess(results, original_shape):\n            \"\"\"\n            Post-process model results for ROS message format\n            \"\"\"\n            height, width = original_shape[:2]\n            \n            processed_results = []\n            \n            # Scale results back to original image size\n            scale_x = width / 640.0\n            scale_y = height / 640.0  # Using same dimension if square crop\n            \n            for batch_results in results:\n                batch_processed = []\n                for detection in batch_results:\n                    scaled_bbox = [\n                        detection['bbox'][0] * scale_x,  # x\n                        detection['bbox'][1] * scale_y,  # y\n                        detection['bbox'][2] * scale_x,  # width\n                        detection['bbox'][3] * scale_y   # height\n                    ]\n                    \n                    processed_detection = {\n                        'bbox': scaled_bbox,\n                        'confidence': detection['conf'],\n                        'class': detection['class'],\n                        'centroid': (\n                            scaled_bbox[0] + scaled_bbox[2]/2,\n                            scaled_bbox[1] + scaled_bbox[3]/2\n                        )\n                    }\n                    \n                    batch_processed.append(processed_detection)\n                \n                processed_results.append(batch_processed)\n            \n            return processed_results\n        \n        return postprocess\n\n    def image_callback(self, msg):\n        \"\"\"\n        Callback for incoming images\n        \"\"\"\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            \n            # Add to processing buffer\n            with self.processing_lock:\n                self.image_buffer.append((msg.header.stamp.sec + msg.header.stamp.nanosec/1e9, cv_image))\n                \n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def imu_callback(self, msg):\n        \"\"\"\n        Callback for incoming IMU data\n        \"\"\"\n        try:\n            # Convert IMU message to internal format\n            imu_data = {\n                'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec/1e9,\n                'orientation': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w],\n                'angular_velocity': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],\n                'linear_acceleration': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]\n            }\n            \n            # Add to sensor buffer\n            with self.processing_lock:\n                self.imu_buffer.append(imu_data)\n                \n        except Exception as e:\n            self.get_logger().error(f'Error processing IMU: {e}')\n\n    def process_callback(self):\n        \"\"\"\n        Processing callback called at regular intervals\n        \"\"\"\n        start_time = time.time()\n        \n        try:\n            # Process image if available\n            if len(self.image_buffer) > 0:\n                # Process with GPU acceleration\n                results = self._process_images()\n                \n                # Publish performance metrics\n                process_time = time.time() - start_time\n                self.processing_times.append(process_time)\n                \n                if len(self.processing_times) > 0:\n                    avg_time = sum(self.processing_times) / len(self.processing_times)\n                    perf_msg = Float32()\n                    perf_msg.data = 1.0 / avg_time if avg_time > 0 else 0.0\n                    self.perf_pub.publish(perf_msg)\n            \n            # Update frame count for logging\n            self.frame_count += 1\n            if self.frame_count % 100 == 0:\n                self.get_logger().info(f'Processed {self.frame_count} frames')\n                \n        except Exception as e:\n            self.get_logger().error(f'Error in processing callback: {e}')\n\n    def _process_images(self):\n        \"\"\"\n        Process buffered images with GPU acceleration\n        \"\"\"\n        with self.processing_lock:\n            if len(self.image_buffer) == 0:\n                return []\n            \n            # Get latest image for processing\n            timestamp, image = self.image_buffer[-1]  # Get most recent image\n        \n        try:\n            # Preprocess image\n            input_tensor = self.preprocessor(image).unsqueeze(0)\n            \n            # Move to appropriate device\n            if self.gpu_available:\n                input_tensor = input_tensor.to(self.device)\n            \n            # Run inference with timing\n            with torch.no_grad():\n                if self.gpu_available and torch.cuda.is_available():\n                    torch.cuda.synchronize()  # Ensure GPU operations complete\n                \n                inference_start = time.time()\n                results = self.model(input_tensor)\n                \n                if self.gpu_available and torch.cuda.is_available():\n                    torch.cuda.synchronize()  # Wait for GPU operations to complete\n                \n                inference_time = time.time() - inference_start\n            \n            # Post-process results\n            processed_results = self.postprocessor(results, image.shape)\n            \n            # Log performance metrics\n            self.get_logger().debug(f'Inference time: {inference_time:.3f}s')\n            \n            # If GPU is available, report GPU utilization\n            if self.gpu_available:\n                gpu_status = self.memory_manager.get_status()\n                self.get_logger().debug(\n                    f'GPU: {gpu_status[\"utilization\"]:.1%} utilized, '\n                    f'{gpu_status[\"pool_size\"]} tensors in pool'\n                )\n            \n            # Perform action based on results (example: obstacle avoidance)\n            self._perform_action_from_detections(processed_results[0])\n            \n            return processed_results\n            \n        except Exception as e:\n            self.get_logger().error(f'Error in image processing: {e}')\n            return []\n\n    def _perform_action_from_detections(self, detections):\n        \"\"\"\n        Perform robotic action based on object detections\n        \"\"\"\n        # Example: Simple obstacle avoidance based on object proximity\n        twist_cmd = Twist()\n        \n        # Check if any high-confidence objects are detected nearby\n        obstacle_detected = False\n        for detection in detections:\n            if detection['confidence'] > self.get_parameter('confidence_threshold').value:\n                # Simple rule: if object is in center of image and close, turn\n                img_center_x = 320  # Assuming 640x480 image\n                centroid_x = detection['centroid'][0]\n                \n                # If object is in center third of image\n                if abs(centroid_x - img_center_x) < img_center_x / 3:\n                    obstacle_detected = True\n                    break\n        \n        # If obstacle detected, turn away\n        if obstacle_detected:\n            twist_cmd.angular.z = 0.5  # Turn right\n            twist_cmd.linear.x = 0.0   # Stop forward motion\n        else:\n            twist_cmd.linear.x = 0.2   # Move forward\n            twist_cmd.angular.z = 0.0  # Don't turn\n        \n        # Publish command\n        self.vel_cmd_pub.publish(twist_cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AdvancedIsaacROSNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"custom-isaac-ros-integration",children:"Custom Isaac ROS Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist, PoseWithCovarianceStamped\nfrom std_msgs.msg import Header\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport torch\nimport threading\nfrom collections import deque\nimport time\n\nclass CustomIsaacROSIntegrationNode(Node):\n    def __init__(self):\n        super().__init__('custom_isaac_ros_integration_node')\n        \n        # Parameters\n        self.declare_parameter('fusion_rate', 15.0)  # Hz\n        self.declare_parameter('max_fusion_delay', 0.2)  # seconds\n        self.declare_parameter('confidence_threshold', 0.7)\n        \n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n        self.laser_sub = self.create_subscription(\n            LaserScan, '/scan', self.laser_callback, 10\n        )\n        self.pose_sub = self.create_subscription(\n            PoseWithCovarianceStamped, '/amcl_pose', self.pose_callback, 10\n        )\n        \n        self.fused_output_pub = self.create_publisher(\n            MarkerArray, '/isaac_ros/fused_perception', 10\n        )\n        self.control_cmd_pub = self.create_publisher(\n            Twist, '/cmd_vel', 10\n        )\n        \n        # Initialize components\n        self.bridge = CvBridge()\n        self.sync_manager = IsaacROSSyncManager(\n            max_sync_delay=self.get_parameter('max_fusion_delay').value\n        )\n        self.fusion_component = PerceptionFusionComponent(\n            confidence_threshold=self.get_parameter('confidence_threshold').value\n        )\n        \n        # Data buffers\n        self.image_buffer = deque(maxlen=5)\n        self.laser_buffer = deque(maxlen=5)\n        self.pose_buffer = deque(maxlen=5)\n        \n        # Processing thread\n        self.processing_thread = threading.Thread(target=self._processing_loop)\n        self.processing_active = True\n        self.processing_lock = threading.Lock()\n        \n        # Performance metrics\n        self.fusion_count = 0\n        self.last_fusion_time = time.time()\n        \n        # Start processing thread\n        self.processing_thread.start()\n        \n        self.get_logger().info('Custom Isaac ROS Integration Node initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image messages\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec / 1e9\n            \n            with self.processing_lock:\n                self.image_buffer.append((timestamp, cv_image))\n                \n            # Add to sync manager for potential fusion\n            self.sync_manager.add_message('/camera/image_raw', (timestamp, cv_image))\n            \n        except Exception as e:\n            self.get_logger().error(f'Error in image callback: {e}')\n\n    def laser_callback(self, msg):\n        \"\"\"Process incoming laser scan messages\"\"\"\n        try:\n            timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec / 1e9\n            laser_data = {\n                'ranges': np.array(msg.ranges),\n                'intensities': np.array(msg.intensities),\n                'header': msg.header\n            }\n            \n            with self.processing_lock:\n                self.laser_buffer.append((timestamp, laser_data))\n                \n            # Add to sync manager\n            self.sync_manager.add_message('/scan', (timestamp, laser_data))\n            \n        except Exception as e:\n            self.get_logger().error(f'Error in laser callback: {e}')\n\n    def pose_callback(self, msg):\n        \"\"\"Process incoming pose messages\"\"\"\n        try:\n            timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec / 1e9\n            pose_data = {\n                'position': [msg.pose.pose.position.x, msg.pose.pose.position.y, msg.pose.pose.position.z],\n                'orientation': [msg.pose.pose.orientation.x, msg.pose.pose.orientation.y, \n                              msg.pose.pose.orientation.z, msg.pose.pose.orientation.w],\n                'covariance': list(msg.pose.covariance)\n            }\n            \n            with self.processing_lock:\n                self.pose_buffer.append((timestamp, pose_data))\n                \n            # Add to sync manager\n            self.sync_manager.add_message('/amcl_pose', (timestamp, pose_data))\n            \n        except Exception as e:\n            self.get_logger().error(f'Error in pose callback: {e}')\n\n    def _processing_loop(self):\n        \"\"\"Background processing loop\"\"\"\n        while self.processing_active:\n            try:\n                self._fuse_sensor_data()\n                time.sleep(1.0 / self.get_parameter('fusion_rate').value)\n            except Exception as e:\n                self.get_logger().error(f'Error in processing loop: {e}')\n                time.sleep(0.1)  # Brief pause if error occurs\n\n    def _fuse_sensor_data(self):\n        \"\"\"Fuse data from multiple sensors\"\"\"\n        with self.processing_lock:\n            if len(self.image_buffer) == 0 or len(self.laser_buffer) == 0:\n                return  # Need both image and laser data\n            \n            # Get most recent data\n            img_ts, image = self.image_buffer[-1]\n            laser_ts, laser_data = self.laser_buffer[-1]\n            pose_ts, pose_data = (self.pose_buffer[-1] if self.pose_buffer else (0, None))\n        \n        # Perform sensor fusion\n        fusion_result = self.fusion_component.fuse_data(\n            image=image,\n            laser=laser_data,\n            pose=pose_data,\n            timestamps={'image': img_ts, 'laser': laser_ts, 'pose': pose_ts}\n        )\n        \n        # Publish fused results\n        if fusion_result:\n            marker_array = self._create_fusion_markers(fusion_result)\n            if marker_array.markers:\n                self.fused_output_pub.publish(marker_array)\n                \n                # Update metrics\n                self.fusion_count += 1\n                if self.fusion_count % 100 == 0:\n                    self.get_logger().info(f'Performed fusion {self.fusion_count} times')\n        \n        # Perform action based on fusion result\n        self._act_on_fusion_result(fusion_result)\n\n    def _create_fusion_markers(self, fusion_result):\n        \"\"\"Create visualization markers from fusion result\"\"\"\n        marker_array = MarkerArray()\n        \n        if not fusion_result:\n            return marker_array\n        \n        # Create markers for detected objects\n        for i, obj in enumerate(fusion_result.get('detected_objects', [])):\n            # Create object marker\n            marker = Marker()\n            marker.header = Header()\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.header.frame_id = \"map\"  # or appropriate frame\n            \n            marker.ns = \"fused_objects\"\n            marker.id = i\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n            \n            # Position from fusion (might use pose and image data)\n            if 'position' in obj:\n                marker.pose.position.x = obj['position'][0]\n                marker.pose.position.y = obj['position'][1]\n                marker.pose.position.z = obj['position'][2]\n            \n            # Orientation\n            marker.pose.orientation.w = 1.0  # Default orientation\n            \n            # Scale\n            marker.scale.x = 0.3\n            marker.scale.y = 0.3\n            marker.scale.z = 0.3\n            \n            # Color based on confidence\n            confidence = obj.get('confidence', 0.5)\n            marker.color.r = 1.0 - confidence\n            marker.color.g = confidence\n            marker.color.b = 0.2\n            marker.color.a = 0.8\n            \n            marker.lifetime = rclpy.duration.Duration(seconds=2).to_msg()\n            \n            marker_array.markers.append(marker)\n        \n        return marker_array\n\n    def _act_on_fusion_result(self, fusion_result):\n        \"\"\"Act based on fusion results\"\"\"\n        if not fusion_result or not fusion_result.get('detected_objects'):\n            # If no relevant objects, continue forward\n            cmd = Twist()\n            cmd.linear.x = 0.3  # Continue forward\n            self.control_cmd_pub.publish(cmd)\n            return\n        \n        # Example behavior: avoid close objects\n        dangerous_objects = [\n            obj for obj in fusion_result['detected_objects']\n            if obj.get('confidence', 0) > self.get_parameter('confidence_threshold').value\n            and self._is_threat_to_navigation(obj)\n        ]\n        \n        cmd = Twist()\n        \n        if dangerous_objects:\n            # If threats detected, stop and turn\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.5  # Turn away\n        else:\n            # Safe to continue\n            cmd.linear.x = 0.3\n            cmd.angular.z = 0.0\n        \n        self.control_cmd_pub.publish(cmd)\n\n    def _is_threat_to_navigation(self, obj):\n        \"\"\"Determine if an object poses a navigation threat\"\"\"\n        # Check if object is close and in path\n        if 'position' in obj:\n            x, y, z = obj['position']\n            distance = np.sqrt(x**2 + y**2)\n            \n            # Consider objects within 1m as potential threats\n            return distance < 1.0\n        \n        return False\n\n    def destroy_node(self):\n        \"\"\"Clean up before destroying the node\"\"\"\n        self.processing_active = False\n        if self.processing_thread.is_alive():\n            self.processing_thread.join(timeout=2.0)\n        super().destroy_node()\n\nclass PerceptionFusionComponent:\n    def __init__(self, confidence_threshold=0.7):\n        self.confidence_threshold = confidence_threshold\n        \n    def fuse_data(self, image, laser, pose, timestamps):\n        \"\"\"\n        Fuse data from different sensors to create unified perception\n        \n        Args:\n            image: Latest image data\n            laser: Latest laser scan data\n            pose: Current robot pose (if available)\n            timestamps: Timestamps for data synchronization\n        \n        Returns:\n            Fused perception result\n        \"\"\"\n        # Placeholder fusion implementation\n        # In a real system, this would:\n        # 1. Associate image detections with laser returns\n        # 2. Use pose to transform sensor data to world coordinates\n        # 3. Perform data association and tracking\n        # 4. Apply uncertainty propagation\n        \n        # For this example, we'll simulate fusion\n        fused_result = {\n            'timestamp': time.time(),\n            'detected_objects': [],\n            'environment_map': None,\n            'navigation_relevance': True\n        }\n        \n        # Simulate object detection from image\n        image_objects = self._detect_from_image(image)\n        \n        # Simulate object detection from laser\n        laser_objects = self._detect_from_laser(laser)\n        \n        # Associate objects between modalities\n        associated_objects = self._associate_objects(image_objects, laser_objects, pose)\n        \n        fused_result['detected_objects'] = [\n            obj for obj in associated_objects \n            if obj.get('confidence', 0) > self.confidence_threshold\n        ]\n        \n        return fused_result\n    \n    def _detect_from_image(self, image):\n        \"\"\"Detect objects from image data\"\"\"\n        # Simulate image-based object detection\n        # In a real system, this would run a deep learning model\n        detected_objects = []\n        \n        # For simulation, create mock detections\n        for i in range(np.random.randint(1, 4)):\n            obj = {\n                'type': np.random.choice(['person', 'car', 'obstacle']),\n                'confidence': np.random.uniform(0.5, 0.95),\n                'pixel_coords': [np.random.randint(0, 640), np.random.randint(0, 480)],\n                'bbox': [np.random.randint(0, 600), np.random.randint(0, 400),\n                        np.random.randint(50, 150), np.random.randint(50, 150)]\n            }\n            detected_objects.append(obj)\n        \n        return detected_objects\n    \n    def _detect_from_laser(self, laser_data):\n        \"\"\"Detect objects from laser data\"\"\"\n        # Simulate laser-based object detection\n        # In a real system, this would cluster laser returns\n        detected_objects = []\n        \n        # For simulation, cluster laser returns\n        ranges = laser_data['ranges']\n        valid_ranges = ~np.isnan(ranges) & (ranges > 0) & (ranges < 10)  # Valid ranges to 10m\n        \n        # Simple clustering of laser returns\n        clusters = self._cluster_laser_returns(ranges[valid_ranges])\n        \n        for cluster in clusters[:3]:  # Limit to 3 largest clusters\n            if len(cluster) > 3:  # At least 3 points to be valid\n                avg_range = np.mean([ranges[i] for i in cluster])\n                avg_angle = np.mean([laser_data['header'].angle_min + i * \n                                    laser_data['header'].angle_increment for i in cluster])\n                \n                # Convert polar to cartesian\n                x = avg_range * np.cos(avg_angle)\n                y = avg_range * np.sin(avg_angle)\n                \n                obj = {\n                    'type': 'obstacle',\n                    'confidence': min(0.95, 0.3 + len(cluster) * 0.1),  # Higher confidence for larger clusters\n                    'position': [float(x), float(y), 0.0],\n                    'size': len(cluster)\n                }\n                detected_objects.append(obj)\n        \n        return detected_objects\n    \n    def _cluster_laser_returns(self, ranges):\n        \"\"\"Cluster nearby laser returns\"\"\"\n        clusters = []\n        current_cluster = []\n        \n        for i, r in enumerate(ranges):\n            if current_cluster:\n                # Check if this point is close to the last in cluster\n                last_idx = current_cluster[-1]\n                # For simplicity, check if consecutive and close\n                if i == last_idx + 1 and abs(ranges[last_idx] - r) < 0.3:\n                    current_cluster.append(i)\n                else:\n                    if len(current_cluster) > 1:  # Only keep clusters with multiple points\n                        clusters.append(current_cluster)\n                    current_cluster = [i]\n            else:\n                current_cluster = [i]\n        \n        if len(current_cluster) > 1:\n            clusters.append(current_cluster)\n        \n        return clusters\n    \n    def _associate_objects(self, image_objects, laser_objects, pose):\n        \"\"\"Associate objects detected by different sensors\"\"\"\n        # For this simulation, we'll create associations based on rough geometric matching\n        # In a real system, this would involve proper coordinate transformation\n        # and data association algorithms\n        \n        associated_objects = []\n        \n        # Add image-only objects with pixel coordinates transformed to estimated world coords\n        for img_obj in image_objects:\n            associated_obj = {\n                'type': img_obj['type'],\n                'confidence': img_obj['confidence'],\n                'source_modality': 'image',\n                'pixel_coords': img_obj['pixel_coords']\n            }\n            \n            # If we have pose, estimate world coordinates (simplified)\n            if pose:\n                # This is a highly simplified transformation\n                # Real implementation would use camera calibration and robot pose\n                associated_obj['position'] = [\n                    pose['position'][0] + np.random.uniform(-1.0, 1.0),  # Estimated position\n                    pose['position'][1] + np.random.uniform(-1.0, 1.0),\n                    pose['position'][2]\n                ]\n            \n            associated_objects.append(associated_obj)\n        \n        # Add laser-only objects with their world coordinates\n        for laser_obj in laser_objects:\n            # Transform laser object coordinates to global frame if pose is available\n            if pose:\n                # Simple transformation (translation only for this example)\n                global_pos = [\n                    pose['position'][0] + laser_obj['position'][0],\n                    pose['position'][1] + laser_obj['position'][1],\n                    pose['position'][2] + laser_obj['position'][2]\n                ]\n                laser_obj['position'] = global_pos\n                laser_obj['source_modality'] = 'laser'\n            \n            associated_objects.append(laser_obj)\n        \n        # Look for potential matches between image and laser objects\n        # For now, just keep them separate\n        \n        return associated_objects\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CustomIsaacROSIntegrationNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-sim-integration",children:"Isaac Sim Integration"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-sim-to-ros-bridge",children:"Isaac Sim to ROS Bridge"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class IsaacSimROSBridge:\n    def __init__(self):\n        """\n        Bridge between Isaac Sim and ROS for simulation-to-deployment\n        """\n        self.sim_world = None\n        self.ros_node = None\n        self.bridge_components = {}\n        \n    def setup_isaac_sim_environment(self):\n        """\n        Setup Isaac Sim components to interface with ROS\n        """\n        # This would connect to an Isaac Sim environment\n        # For this example, we\'ll create a conceptual implementation:\n        \n        # 1. Create sensors in Isaac Sim\n        # 2. Configure them to publish to virtual ROS topics\n        # 3. Set up TF broadcasters between Isaac Sim and ROS frames\n        \n        print("Setting up Isaac Sim environment for ROS integration...")\n        \n        # In a real implementation, this would involve:\n        # - Loading robot models into Isaac Sim\n        # - Attaching sensors (cameras, LiDAR, IMU) to the robot\n        # - Configuring these sensors to publish ROS messages\n        # - Setting up the physics simulation environment\n        \n        return True\n    \n    def setup_ros_side_bridge(self):\n        """\n        Setup ROS nodes to receive messages from Isaac Sim\n        """\n        # Initialize ROS components that correspond to Isaac Sim sensors\n        # This would typically be handled by the Isaac ROS bridge packages\n        \n        print("Setting up ROS-side bridge components...")\n        \n        # In a real implementation, this would involve:\n        # - Initializing the ROS-Isaac bridge\n        # - Setting up message bridges for sensor data\n        # - Creating TF broadcasters for robot state\n        # - Setting up action/trajectory interfaces for control\n        \n        return True\n    \n    def start_bridge(self):\n        """\n        Start the full Isaac Sim-ROS bridge operation\n        """\n        if not self.setup_isaac_sim_environment():\n            raise RuntimeError("Failed to setup Isaac Sim environment")\n        \n        if not self.setup_ros_side_bridge():\n            raise RuntimeError("Failed to setup ROS bridge components")\n        \n        print("Isaac Sim-ROS bridge running...")\n        \n    def update_bridge(self):\n        """\n        Update the bridge to synchronize Isaac Sim and ROS states\n        """\n        # This would synchronize:\n        # - Robot joint states\n        # - Sensor readings (camera, LiDAR, IMU)\n        # - TF transforms\n        # - Physics simulation state\n        \n        pass\n\n# Example launch file for the bridge\n"""\n# isaac_sim_ros_bridge.launch.py\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Arguments\n    use_sim_time = LaunchConfiguration(\'use_sim_time\', default=\'true\')\n    \n    # Isaac Sim Bridge Node\n    isaac_sim_bridge = Node(\n        package=\'omni.isaac.ros_bridge\',\n        executable=\'isaac_ros_bridge\',\n        name=\'isaac_sim_bridge\',\n        parameters=[\n            {\'use_sim_time\': use_sim_time}\n        ],\n        remappings=[\n            (\'/isaac_sim/joint_states\', \'/joint_states\'),\n            (\'/isaac_sim/imu\', \'/imu/data\'),\n            (\'/isaac_sim/camera_rgb\', \'/camera/image_raw\'),\n            (\'/isaac_sim/lidar\', \'/scan\')\n        ]\n    )\n    \n    return LaunchDescription([\n        DeclareLaunchArgument(\n            \'use_sim_time\',\n            default_value=\'true\',\n            description=\'Use simulation time\'\n        ),\n        isaac_sim_bridge\n    ])\n"""\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization-techniques",children:"Performance Optimization Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"gpu-memory-management-1",children:"GPU Memory Management"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class AdvancedGPUPerformanceOptimizer:\n    def __init__(self, target_fps=30, max_latency=0.1):\n        \"\"\"\n        Optimize GPU usage for Isaac ROS integration\n        \n        Args:\n            target_fps: Target frame rate for processing\n            max_latency: Maximum acceptable processing latency (seconds)\n        \"\"\"\n        self.target_fps = target_fps\n        self.max_latency = max_latency\n        self.performance_metrics = {\n            'current_fps': 0,\n            'avg_latency': 0,\n            'gpu_utilization': 0,\n            'gpu_memory_usage': 0\n        }\n        self.adaptive_params = {\n            'batch_size': 1,\n            'input_resolution': (640, 640),\n            'model_precision': 'fp32'  # or 'fp16'\n        }\n    \n    def optimize_processing_pipeline(self, pipeline_func):\n        \"\"\"\n        Wrap a processing pipeline with optimization strategies\n        \"\"\"\n        def optimized_wrapper(*args, **kwargs):\n            start_time = time.time()\n            \n            # Adjust parameters based on performance\n            self._adjust_parameters_based_on_load()\n            \n            result = pipeline_func(*args, **kwargs)\n            \n            end_time = time.time()\n            \n            # Update performance metrics\n            self._update_performance_metrics(end_time - start_time)\n            \n            # Adjust for real-time requirements\n            self._enforce_realtime_constraints()\n            \n            return result\n        \n        return optimized_wrapper\n    \n    def _adjust_parameters_based_on_load(self):\n        \"\"\"\n        Adjust processing parameters based on current load\n        \"\"\"\n        if self.performance_metrics['current_fps'] < self.target_fps * 0.8:\n            # System is overloaded - reduce quality settings\n            if self.adaptive_params['model_precision'] == 'fp32':\n                self.adaptive_params['model_precision'] = 'fp16'\n            \n            if self.adaptive_params['input_resolution'] == (640, 640):\n                self.adaptive_params['input_resolution'] = (416, 416)\n            \n            if self.adaptive_params['batch_size'] > 1:\n                self.adaptive_params['batch_size'] = max(1, self.adaptive_params['batch_size'] - 1)\n        \n        elif self.performance_metrics['current_fps'] > self.target_fps * 1.2:\n            # System has capacity - increase quality settings\n            if self.adaptive_params['model_precision'] == 'fp16' and self._supports_fp16():\n                self.adaptive_params['model_precision'] = 'fp32'\n            \n            if self.adaptive_params['input_resolution'] == (416, 416):\n                self.adaptive_params['input_resolution'] = (640, 640)\n    \n    def _update_performance_metrics(self, processing_time):\n        \"\"\"\n        Update performance metrics based on latest processing\n        \"\"\"\n        self.performance_metrics['avg_latency'] = 0.9 * self.performance_metrics['avg_latency'] + 0.1 * processing_time\n        self.performance_metrics['current_fps'] = 0.9 * self.performance_metrics['current_fps'] + 0.1 * (1.0 / processing_time)\n        \n        # Update GPU metrics\n        if torch.cuda.is_available():\n            self.performance_metrics['gpu_utilization'] = torch.cuda.utilization()\n            self.performance_metrics['gpu_memory_usage'] = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory\n    \n    def _enforce_realtime_constraints(self):\n        \"\"\"\n        Ensure real-time processing requirements are met\n        \"\"\"\n        if self.performance_metrics['avg_latency'] > self.max_latency:\n            # Drop frames if we're falling behind\n            print(f\"Warning: Latency {self.performance_metrics['avg_latency']:.3f}s exceeds max {self.max_latency}s\")\n    \n    def _supports_fp16(self):\n        \"\"\"\n        Check if current GPU supports FP16 precision\n        \"\"\"\n        if torch.cuda.is_available():\n            prop = torch.cuda.get_device_properties(0)\n            return prop.major >= 7  # FP16 supported on compute capability 7.0+\n        return False\n"})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,t.jsx)(n.h3,{id:"hardware-acceleration-problems",children:"Hardware Acceleration Problems"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"GPU Memory Exhaustion"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Monitor GPU memory usage during operation"}),"\n",(0,t.jsx)(n.li,{children:"Use mixed precision (FP16/FP32) where possible"}),"\n",(0,t.jsx)(n.li,{children:"Implement proper memory cleanup routines"}),"\n",(0,t.jsx)(n.li,{children:"Batch process when memory allows"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Synchronization Errors"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ensure proper timestamp management"}),"\n",(0,t.jsx)(n.li,{children:"Use appropriate sync policies for different sensor types"}),"\n",(0,t.jsx)(n.li,{children:"Implement fallback strategies for unsynchronized data"}),"\n",(0,t.jsx)(n.li,{children:"Monitor timing constraints for real-time performance"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Performance Bottlenecks"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Profile GPU vs CPU usage"}),"\n",(0,t.jsx)(n.li,{children:"Optimize data transfers between devices"}),"\n",(0,t.jsx)(n.li,{children:"Use CUDA streams for overlapping operations"}),"\n",(0,t.jsx)(n.li,{children:"Consider model optimization (quantization, pruning)"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"integration-challenges",children:"Integration Challenges"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Message Rate Mismatch"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use message filters to handle different rates"}),"\n",(0,t.jsx)(n.li,{children:"Implement buffering for smoothing"}),"\n",(0,t.jsx)(n.li,{children:"Use appropriate throttling mechanisms"}),"\n",(0,t.jsx)(n.li,{children:"Consider sensor fusion timing requirements"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Coordinate System Issues"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Verify TF transforms between frames"}),"\n",(0,t.jsx)(n.li,{children:"Ensure consistent units and conventions"}),"\n",(0,t.jsx)(n.li,{children:"Validate sensor mounting positions"}),"\n",(0,t.jsx)(n.li,{children:"Check calibration accuracy"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Communication Failures"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement retry mechanisms"}),"\n",(0,t.jsx)(n.li,{children:"Set appropriate timeouts"}),"\n",(0,t.jsx)(n.li,{children:"Monitor network performance"}),"\n",(0,t.jsx)(n.li,{children:"Use reliable QoS profiles when needed"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"architecture-design",children:"Architecture Design"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modular Components"}),": Design reusable components that can work in different configurations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance Monitoring"}),": Implement comprehensive monitoring from the start"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling"}),": Plan for graceful degradation when components fail"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource Management"}),": Implement proper resource allocation and cleanup"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"development-workflow",children:"Development Workflow"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulation-Reality Gap"}),": Test in simulation before real hardware"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Incremental Integration"}),": Integrate components one at a time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Profiling"}),": Continuously profile performance during development"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Version Control"}),": Maintain versions of both software and models"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware Requirements"}),": Clearly specify needed hardware configurations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Calibration"}),": Document sensor calibration procedures"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Maintenance"}),": Plan for model updates and system maintenance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety"}),": Implement safety mechanisms and fallbacks"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Advanced Synchronization"}),": Implement a synchronization system that can handle variable message rates from different sensors."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Performance Optimization"}),": Profile your Isaac ROS integration and implement optimization techniques to improve throughput."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sensor Fusion"}),": Create a fusion node that combines data from camera, LiDAR, and IMU sensors using GPU acceleration."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Isaac Sim Integration"}),": Connect your Isaac ROS system to Isaac Sim to create a complete simulation-to-deployment pipeline."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Fault Tolerance"}),": Add error handling and fallback mechanisms to gracefully handle component failures."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Advanced Isaac-ROS integration requires careful attention to data flow and performance"}),"\n",(0,t.jsx)(n.li,{children:"GPU memory management is critical for maintaining performance"}),"\n",(0,t.jsx)(n.li,{children:"Synchronization between components can be complex but essential for accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Real-time constraints require adaptive processing strategies"}),"\n",(0,t.jsx)(n.li,{children:"Simulation provides a safe environment for testing integration patterns"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"NVIDIA Isaac ROS Micro-Architecture Guide"}),"\n",(0,t.jsx)(n.li,{children:'"Hardware-Accelerated Robotics" - Technical Papers'}),"\n",(0,t.jsx)(n.li,{children:"Isaac Sim ROS Bridge Documentation"}),"\n",(0,t.jsx)(n.li,{children:"CUDA Programming Guide for Robotics Applications"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Continue to Chapter 3: Isaac Sim Integration to explore the connection between Isaac Sim and the Isaac ROS framework for complete simulation-to-deployment workflows."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);